---
layout: post
title: "[paper-review] OFFLINE REINFORCEMENT LEARNING WITH IMPLICIT Q-LEARNING"
date: 2023-10-24
# header-includes:
#    - \usepackage{bbm}
categories:
  - paper-review
  - paper-review/PBL
tags:
  - Reinforcement Learning
  - NeurIPS
  - '2021'
description: "paper review about iql"
use_math: true
classes: wide
giscus_comments: true
related_posts: true
---
> NeurIPS 2023. [[Paper](https://offline-rl-neurips.github.io/2021/pdf/24.pdf)] [[Github<sup>1</sup>](https://github.com/ikostrikov/implicit_q_learning) | [Github<sup>2</sup>](https://github.com/Manchery/iql-pytorch)]
>
> Ilya Kostrikov, Ashvin Nair & Sergey Levine 
> Department of Electrical Engineering and Computer Science University of California, Berkeley
> 
> 12 Oct 2021

## 한 문장 요약
요약: State value function을 random variable로 정의하여, policy improvement를 implicit하게 근사해보자. 구체적으로는 Expectiles of the state value function을 추정해보자.
> Offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization
> 
Keyword: **Reinforcement Learning**, **Offline RL**, **Quantile Regression**

### Introduction

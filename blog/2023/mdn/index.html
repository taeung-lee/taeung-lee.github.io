<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [study] Mixture Density Network | Joel Lee </title> <meta name="author" content="Joel Lee"> <meta name="description" content="Mixture Density Network"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joonhyung-lee.github.io//blog/2023/mdn/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Joel Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">miscellaneous </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/gallery/">gallery</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[study] Mixture Density Network</h1> <p class="post-meta"> October 30, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>     ·   <a href="/blog/category/study"> <i class="fa-solid fa-tag fa-sm"></i> study</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="0-simple-networks">0. Simple Networks</h3> <p>일반적으로 네트워크를 구성한다는 것은 <strong>입력</strong>과 <strong>출력</strong>에 대해 설계하는 것이다. 이를 조건부 확률로 나타내면 다음과 같다.</p> <p>\(p(y|x,z)\)</p> <ul> <li>\(x\): Input of the network</li> <li>\(z\): Model parameter</li> <li>\(y\): Output of the network (=label)</li> </ul> <p>예를 들어, 만일 classification 문제를 수행하고 있다면 \(y\)는 분류해야 할 label을 의미하는 것이다. 각 label은 discrete하게 존재하므로 (e.g., <code class="language-plaintext highlighter-rouge">dog</code> or <code class="language-plaintext highlighter-rouge">cat</code>) 출력 값에 Softmax 연산을 수행해주어 어떠한 class에 해당할 확률을 뱉어주게 된다.</p> <div align="center"> <img src="/assets/img/mdn/mdn-regression.png" width="75%"> <p>Fig. 1: Simple image about Linear Regression.</p> </div> <p>Regression 문제를 수행하고 있다면 출력 값 \(y\)는 continuous 한 값을 갖게 될 것이다. 어떠한 입력 \(x\)에 대응하는 출력 \(y\)의 값이 있을 것이며, 이에 대해 Mean Squared Error (MSE)로 선형회귀를 수행한다면 주어진 입력 \(x\)에 대해 가장 잘 표현하는 직선을 얻게 될 것이다.</p> <p>실제 직선을 \(f(x)\)라고 칭하고, 예측한 직선을 \(\hat{f}(x)\) 라고 한다면 두 직선 간의 차이는 존재하게 된다. \(f(x) = \hat{f}(x) + \epsilon\) 완벽하게 일치하는 직선은 존재할 수 없으며, 작은 오차 \(\epsilon\)을 가지게 되며, 이러한 오차는 (보통) Gaussian distribution을 따른다고 가정한다. 이를 확률 분포로 표현하면 아래와 같으며, \(w\)는 각각의 weight를 의미한다.</p> \[\left( y \mid x \right) \sim N(w^Tx, \sigma^2)\] <h4 id="01-example-simple-sinusoidal-function">0.1. Example: Simple Sinusoidal Function</h4> \[\begin{equation} y_{\text{true}}(x) = 7\sin{(0.75x)} + 0.5x + \epsilon \end{equation}\] <p>위 함수를 기준으로 Regression 문제를 수행해보자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">10.5</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y_data</span> <span class="o">=</span> <span class="mi">7</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mf">0.75</span><span class="o">*</span><span class="n">x_data</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x_data</span> <span class="o">+</span> <span class="n">epsilon</span>
    <span class="k">return</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span> <span class="o">=</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>위 코드를 통해 위 함수에 근사하는 데이터를 추출할 수 있었다.</p> <div align="center"> <img src="/assets/img/mdn/mdn-ex1-func.png" width="50%"> <p>Fig. 2: Sampled from arbitrary function.</p> </div> <p>그럼 이제 이 함수를 근사할 수 있는 간단한 네트워크를 구성해볼 수 있다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_input</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_output</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                        <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">(),</span>
                        <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_output</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">x_data</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_input</span><span class="p">))</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">y_data</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_input</span><span class="p">))</span>
<span class="n">x_variable</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">)</span>
<span class="n">y_variable</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">y_tensor</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">network</span><span class="p">(</span><span class="n">x_variable</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_variable</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">300</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <p>간단한 MLP layer를 만들어, MSE Loss를 최소화하도록 학습시킨다. 이때 주의할 점은 numpy array를 pytorch가 사용할 수 있는 tensor로 바꿔줘야한다. 또한 numpy의 기본 형태인 np.float64를 pytorch의 기본형인 np.float32로 바꿔줘야 에러가 발생하지 않는다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">train</span><span class="p">()</span>

<span class="n">x_test_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">x_test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_input</span><span class="p">))</span>
<span class="n">x_test_variable</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">x_test_tensor</span><span class="p">)</span>
<span class="n">y_test_variable</span> <span class="o">=</span> <span class="nf">network</span><span class="p">(</span><span class="n">x_test_variable</span><span class="p">)</span>
<span class="n">y_test_data</span> <span class="o">=</span> <span class="n">y_test_variable</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">,</span> <span class="n">y_test_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div align="center"> <img src="/assets/img/mdn/mdn-ex1-func-fitted.png" width="50%"> <p>Fig. 3: Fitted with simple network.</p> </div> <p>이는 간단하게 학습된 네트워크에 test input \(x\)를 샘플하여 정말 잘 학습 되었는지 확인하는 코드이다. 위 그림을 보면 알 수 있듯이, 손쉽게 해당 function에 fitting 된 것을 알 수 있다. 이론 상으로는 은닉층 하나 만으로도 MLP는 임의의 함수를 근사할 수 있다는 점을 알 수 있다. 즉, 하나 이상의 input \(x\)와 그에 대응하는 오직 하나의 출력 \(y\)을 갖는 함수에 대해서는 쉽게 표현할 수 있다는 것을 의미한다.</p> <p>다르게 생각해보면, 여러 개의 출력을 갖는 경우에는 어떻게 될지 생각해볼 수 있다. 즉 앞선 경우는 mode가 1인 경우에 regression을 수행한 것이며, 여러 개의 mode로 표현되는 정규분포에 대해 regression을 수행할 때에도 정말 잘 표현할 수 있는지는 고민해볼 필요가 있다.</p> <h4 id="02-example-reverse-of-the-eqn-1">0.2. Example: Reverse of the <a href="#equation-1">Eqn. 1</a> </h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">y_data</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div align="center"> <img src="/assets/img/mdn/mdn-ex2-func.png" width="50%"> <p>Fig. 4: Reverse of the function.</p> </div> <p>간단하게 \(x\)와 \(y\)를 서로 바꾸어주면 된다. 이렇게 되면 하나의 입력 \(x\)는 여러 개의 출력 \(y\)를 표현하게 된다. 이러한 상황에서 기존의 네트워크를 사용하여 다시 학습해보자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_variable</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">y_tensor</span>
<span class="n">y_variable</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">x_tensor</span>

<span class="nf">train</span><span class="p">()</span>

<span class="n">x_test_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">x_test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_input</span><span class="p">))</span>
<span class="n">x_test_variable</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">x_test_tensor</span>

<span class="n">y_test_variable</span> <span class="o">=</span> <span class="nf">network</span><span class="p">(</span><span class="n">x_test_variable</span><span class="p">)</span>

<span class="c1"># move from torch back to numpy
</span><span class="n">y_test_data</span> <span class="o">=</span> <span class="n">y_test_variable</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># plot the original data and the test data
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">y_data</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">,</span> <span class="n">y_test_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div align="center"> <img src="/assets/img/mdn/mdn-ex2-func-fitted.png" width="50%"> <p>Fig. 5: Result with simple network.</p> </div> <p>이렇게 여러 mode의 distribution에 대해서는 일반적인(간단한) 네트워크로는 표현하기 어렵다는 것을 알 수 있다. 이는 기본적으로 MSE Loss를 최소화하도록 학습시키고 각 입력에 대해 하나의 출력만 가능했기 때문이다. 이러한 문제를 해결하기 위해서 제안된 것이 <strong>Mixture Density Network (MDN)</strong> 이다.</p> <h3 id="1-mixture-density-network-mdn">1. Mixture Density Network (MDN)</h3> <h4 id="11-definition">1.1 Definition</h4> <p>MDN은 <a href="https://www.google.com/search?q=Christopher+Bishop+MDN&amp;sca_esv=577843258&amp;ei=g88_ZeriHpnM1e8P_s64sAM&amp;ved=0ahUKEwiqsKHhjp6CAxUZZvUHHX4nDjYQ4dUDCBA&amp;uact=5&amp;oq=Christopher+Bishop+MDN&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiFkNocmlzdG9waGVyIEJpc2hvcCBNRE4yBRAhGKABMgUQIRigATIFECEYoAEyBRAhGKABSIYNUCFYpAxwBngBkAEAmAGWAaAB_gWqAQMwLja4AQPIAQD4AQHCAgoQABhHGNYEGLADwgIFEAAYgATCAgQQABgewgINEC4YExicAxioAxiABMICBxAAGBMYgATCAg0QLhgTGIAEGKgDGJoDwgIcEC4YExicAxioAxiABBiXBRjcBBjeBBjgBNgBAcICCRAhGKABGAoYKsICBxAhGKABGArCAg0QLhgTGIAEGJoDGKgDwgIGEAAYHhgTwgIIEAAYCBgeGBPiAwQYACBBiAYBkAYHugYGCAEQARgU&amp;sclient=gws-wiz-serp" rel="external nofollow noopener" target="_blank">paper</a>에서 자세한 내용을 확인할 수 있다. 정리하면, 하나의 입력 \(x\)에 대해 다른 분포를 가지는 \(y\)에서 \(p(y|x)\)를 추정하는 것이다.</p> \[p(y\mid x)=\sum^{n}_{i=1} p(c=i\mid x) ~ \mathcal{N}(y;\mu^{i}, \sigma^{i})\] <p>이를 수식으로 표현하면 위와 같으며, \(n\)개의 정규분포를 가정하고 각 분포에서 \(y\)가 나올 확률을 이 분포에 속할 확률과 곱하여 결과를 예측하는 것이다. 이렇게 \(p(y \mid x)\)를 얻어, 여기서 sample 하여 함수를 추정하면 끝이다.</p> <p>그렇게 Network에서 결정(추정)해주어야 할 값은 총 3가지 이다. (각 분포마다 3개씩.)</p> <ul> <li>\(p(c=i \mid x)\): 입력 \(x\)가 category \(i\)에 속할 확률. <ul> <li>확률분포의 정의에 의해 Normalize를 반드시 해주어야 한다. (총 합이 1이 되게끔 softmax를 수행함.)</li> </ul> </li> <li>\(\mu^{i},~\sigma^{i}\): category \(i\)에 속할 때 \(y\)가 따르는 정규분포 <ul> <li> \[\sigma^{i} &gt; 0\] </li> </ul> </li> </ul> <p>그리고 이젠 출력이 단일한 값을 뱉어주지 못하므로 loss 또한 MSE Loss가 아닌 다른 loss를 사용해야 한다. Cross-Entropy Loss를 사용하며, 수식은 아래와 같다.</p> \[\begin{equation} \mathcal{L}_{\text{C.E.}} = -\log{ \sum_{i=1}^m p(c = i \mid x)N(y; \mu^i, \sigma^i)} \end{equation}\] <p>그렇다면 위 loss function을 가지며, Network의 출력은 \(\pi,~\mu,~\sigma\)를 하는 MDN class를 만든다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MDN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_gaussians</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MDN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">z_h</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">z_pi</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_gaussians</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">z_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_gaussians</span><span class="p">)</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_gaussians</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z_h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">z_h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">z_pi</span><span class="p">(</span><span class="n">z_h</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">z_sigma</span><span class="p">(</span><span class="n">z_h</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">z_mu</span><span class="p">(</span><span class="n">z_h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pi</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">z_pi</code>는 확률분포 이기에 softmax를 수행해준 것이다.</li> <li> <code class="language-plaintext highlighter-rouge">z_sigma</code>는 항상 양수이어야 하므로 \(\exp\)를 취해주었다.</li> </ul> <p>\(\mu^{i},~\sigma^{i}\)에서 정의되는 정규분포의 수식은 아래와 같다.</p> \[\begin{equation*} \mathcal{N}(\mu, \sigma)(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp (-\frac{(x-\mu)^2}{2\sigma^2}) \end{equation*}\] <p>이를 고려한 loss function을 코드로 구현하면 아래와 같다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gaussian_distribution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">expand_as</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reciprocal</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">result</span> <span class="o">*</span> <span class="n">result</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reciprocal</span><span class="p">(</span><span class="n">sigma</span><span class="p">))</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mdn_loss_fn</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nf">gaussian_distribution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">*</span> <span class="n">pi</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <p>각 분포로부터 \(y\)가 나올 확률 \(\mathcal{N}(\mu,\sigma)(x)\)와 그 분포에 대응할 확률 \(\pi\)을 곱하고, 이 모두를 다 더한 다음, 로그와 평균을 취해주면 된다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">network</span> <span class="o">=</span> <span class="nc">MDN</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_gaussians</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="n">mdn_x_data</span> <span class="o">=</span> <span class="n">y_data</span>
<span class="n">mdn_y_data</span> <span class="o">=</span> <span class="n">x_data</span>

<span class="n">mdn_x_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span>
<span class="n">mdn_y_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span>

<span class="n">x_variable</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">mdn_x_tensor</span><span class="p">)</span>
<span class="n">y_variable</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">mdn_y_tensor</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_mdn</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">pi_variable</span><span class="p">,</span> <span class="n">sigma_variable</span><span class="p">,</span> <span class="n">mu_variable</span> <span class="o">=</span> <span class="nf">network</span><span class="p">(</span><span class="n">x_variable</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">mdn_loss_fn</span><span class="p">(</span><span class="n">pi_variable</span><span class="p">,</span> <span class="n">sigma_variable</span><span class="p">,</span> <span class="n">mu_variable</span><span class="p">,</span> <span class="n">y_variable</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="nf">train_mdn</span><span class="p">()</span>

<span class="n">pi_variable</span><span class="p">,</span> <span class="n">sigma_variable</span><span class="p">,</span> <span class="n">mu_variable</span> <span class="o">=</span> <span class="nf">network</span><span class="p">(</span><span class="n">x_test_variable</span><span class="p">)</span>

<span class="n">pi_data</span> <span class="o">=</span> <span class="n">pi_variable</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">sigma_data</span> <span class="o">=</span> <span class="n">sigma_variable</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mu_data</span> <span class="o">=</span> <span class="n">mu_variable</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">,</span> <span class="n">pi_data</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">$$p(c = i | x)$$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">,</span> <span class="n">sigma_data</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">$$\sigma$$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">,</span> <span class="n">mu_data</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">$$\mu$$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div align="center"> <img src="/assets/img/mdn/mdn-output.png" width="67.5%"> <p>Fig. 6: Result with MDN network.</p> </div> <p>입력 \(x\)의 변화에 따른 각 분포의 양상을 확인할 수 있다. 하나의 \(x\)에 여러 개의 \(y\)가 가능하며, <strong>이 각각의 점이 선택될 확률은</strong> \(\mathbf{p(c=i \mid x)}\)<strong>로 표현되는 것이다.</strong></p> <p>학습시킨 네트워크에서 특정 정규분포의 결과를 얻고 싶다면 <strong><code class="language-plaintext highlighter-rouge">Gumbel softmax</code></strong> sampling을 사용하면 되며, 이론적인 내용은 이후에 작성하겠습니다.</p> <p>이제 우리는 각 입력 \(x\)에 대해서 어떤 정규분포를 선택해야되는지를 알았으니 각각의 평균과 분산을 이용해 이를 sampling만 하면 됩니다. random noise는 표준정규분포를 따르므로 이에 \(\sigma\)를 곱하고 \(\mu\)를 더해주기만하면 원래의 정규분포를 얻을 수 있다. 이렇게 해서 최종 결과물을 시각화하면 아래와 같다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gumbel_sample</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">gumbel</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">z</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

<span class="n">k</span> <span class="o">=</span> <span class="nf">gumbel_sample</span><span class="p">(</span><span class="n">pi_data</span><span class="p">)</span>

<span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
<span class="n">random_noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">sampled</span> <span class="o">=</span> <span class="n">random_noise</span> <span class="o">*</span> <span class="n">sigma_data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span> <span class="o">+</span> <span class="n">mu_data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">mdn_x_data</span><span class="p">,</span> <span class="n">mdn_y_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_test_data</span><span class="p">,</span> <span class="n">sampled</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div align="center"> <img src="/assets/img/mdn/mdn-fitted.png" width="50%"> <p>Fig. 7: Fitted with MDN network.</p> </div> <h3 id="요약">요약</h3> <p>기존의 간단한 network는 <strong>하나의 입력</strong> \(\mathbf{x}\)에 대해 <strong>여러 개의 출력</strong> \(\mathbf{y}\)가 가능한 경우에 효과적으로 표현할 수 없었다. <strong>Mixture Density Network (MDN)</strong>은 <strong>여러 개의 정규분포</strong> \(\mathbf{\mathcal{N}^{i}}\) (혹은 다른 분포)의 <strong>평균</strong> \(\mathbf{\mu}^{i}\)와 <strong>분산</strong> \(\mathbf{\sigma}^{i}\)를 예측하고, <strong>각 정규분포에 속할 확률</strong> \(\mathbf{\pi^{i}=p(c=i \mid x)}\)을 통해 이를 효과적으로 근사할 수 있게 한 것이다. 즉, 여러 개의 출력이 있는 경우에도 분포를 추정할 수 있다는 의미이다.</p> <h5 id="references">References:</h5> <ul> <li><a href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" rel="external nofollow noopener" target="_blank">Paper</a></li> <li><a href="https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb" rel="external nofollow noopener" target="_blank">Code</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/apriltag-pose-estimation/">[구현] AprilTag Pose Estimation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cpl/">[paper-review] Contrastive Prefence Learning: Learning from Human Feedback without RL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/prs/">[paper-review] Poisson Reconstruction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/stow/">[paper-review] Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/rba/">[paper-review] Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joonhyung-lee/joonhyung-lee.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Joel Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
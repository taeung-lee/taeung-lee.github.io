<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [paper-review] Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models | Joel Lee </title> <meta name="author" content="Joel Lee"> <meta name="description" content="paper review about Dream2Real"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joonhyung-lee.github.io//blog/2023/dream2real/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Joel Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">miscellaneous </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/gallery/">gallery</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[paper-review] Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models</h1> <p class="post-meta"> December 09, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/real-to-sim"> <i class="fa-solid fa-hashtag fa-sm"></i> real-to-sim</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reasoning</a>   <a href="/blog/tag/vlm"> <i class="fa-solid fa-hashtag fa-sm"></i> VLM</a>   <a href="/blog/tag/neurips-w"> <i class="fa-solid fa-hashtag fa-sm"></i> NeurIPS-W</a>   <a href="/blog/tag/2023"> <i class="fa-solid fa-hashtag fa-sm"></i> 2023</a>     ·   <a href="/blog/category/paper-review"> <i class="fa-solid fa-tag fa-sm"></i> paper-review</a>   <a href="/blog/category/paper-review-cv"> <i class="fa-solid fa-tag fa-sm"></i> paper-review/cv</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>NeurIPS-W 2023 Oral. [<a href="https://arxiv.org/pdf/2312.04533.pdf" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://www.robot-learning.uk/dream2real" rel="external nofollow noopener" target="_blank">Project Page</a>]</p> <p>Ivan Kapelyukh<sup>1, 2</sup> , Yifei Ren<sup>1</sup> , Ignacio Alzugaray<sup>2</sup> , Edward Johns<sup>1</sup> The Robot Learning Lab at Imperial College London<sup>1</sup>. The Dyson Robotics Lab at Imperial College London<sup>2</sup></p> <p>Dec. 07</p> </blockquote> <div align="center"> <img src="/assets/img/dream2real/overview.png" width="100%"> <p>Fig. 1: Overview of Dream-to-Real.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>NeRF로 TSDF mesh로 만들어 physics check를 하고, Textual+Visual reasoning으로 best pose를 얻어내자.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존의 방법론인 DALEE-E-Bot 과 비교하면 아래의 개선점이 있음. <ul> <li>DALL-E-Bot can only generate 2D images, so is unsuitable for 6-DoF tasks.</li> <li>It is easier to imagine new images by reconfiguring NeRFs, than by learning to generate images.</li> </ul> </li> <li>Zero-shot Manner로 Robotic rearrangement task를 수행할 수 있다. <ul> <li>just <strong>dreaming</strong> and <strong>evaluating</strong> </li> </ul> </li> <li>Robot builds an object-centric NeRF of a scene.</li> <li>Numerous reconfigurations of the scene are rendered as 2D images.</li> <li>CLIP evaluates these according to the user instruction.</li> <li>The best is then physically created using pick-and-place.</li> </ul> <h3 id="methodology">Methodology:</h3> <div align="center"> <img src="/assets/img/dream2real/framework.png" width="100%"> <p>Fig. 2: Framework of Dream-to-Real.</p> </div> <h4 id="build-scene-representation">Build Scene Representation</h4> <ul> <li>RGB-D 카메라로 workspace를 반구 형태로 capture: TSDF Format <ul> <li>Workspace, Target object: NeRF (Instant-NGP)</li> </ul> </li> <li>[SAM+XMem] Segmentatoin VLMs + BLIP-2 <ul> <li>해당 모델로 workspace에 대해 semented workspace + target object (background/foreground)를 얻을 수 있다.</li> <li>각 view의 object에 대해 captioning을 BLIP-2 model로 수행함.</li> </ul> </li> </ul> <h4 id="imagine-best-pose">Imagine Best Pose</h4> <ul> <li>Workspace에 해당하는 Grid로 샘플링 + Orientation도 discrete set 내에서 샘플링 -&gt; 앞서 생성된 TSDF mesh를 기반으로 physics(collision) check <ul> <li><code class="language-plaintext highlighter-rouge">To build the physics models, we combine depth images from across views to create a separate foreground and background Truncated Signed Distance Function (TSDF) [49], which we find achieves more accurate geometry than extracting a mesh from Instant-NGP.</code></li> <li><code class="language-plaintext highlighter-rouge">We move (virtually) the movable object’s physics model to each of the sampled poses in turn and check for physical validity, i.e. the object must not be in collision with the scene or unsupported in free space.</code></li> </ul> </li> <li>GPT-4 Input: [movable object, relevant objects, the goal caption and the normalising caption] <ul> <li>movable object: 직접 움직이게 될 물체를 의미합니다.</li> <li>relevant object: task의 수행 여부를 판단하기 위해 관측해야 하는 object or region을 의미합니다.</li> <li>goal caption: description of the desired final state after the instruction has been fulfilled.</li> <li>normalising caption: spatial relationship을 잘 포착하게끔 해주는 목적이라고 합니다. (<code class="language-plaintext highlighter-rouge">description of the scene that remains neutral to the pose of the object being moved.</code>)</li> </ul> </li> </ul> <h4 id="execute-pick-and-place">Execute Pick-and-Place</h4> <p>Execute Robot action in real-world</p> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/dream2real/environment.png" width="100%"> <p>Fig. 3: Enviroments of Dream2Real.</p> </div> <ul> <li>Setup; 3개의 서로 다른 real-world scene + 10개의 rearrangement task</li> <li>Evaluation metric; Measure task success using success regions.</li> <li>H/W; Franka + D435i</li> </ul> <h4 id="baselines">Baselines</h4> <ul> <li>DALL-E-Bot</li> <li>D2R-One-View, Only use one view: which uses only the first camera view throughout the whole pipeline (including object captioning), avoiding the need for data collection.</li> <li>D2R-Distract: do not use GPT-4 (distractor)</li> <li>Physics-Only: do not use CLIP to evaluate poses</li> <li>D2R-No-Norm: no normalizing captions</li> <li>D2R-Vis-Prior</li> <li>D2R-NoSmooth: ablates spatial smoothing</li> </ul> <h4 id="results">Results</h4> <div align="center"> <img src="/assets/img/dream2real/table1-shopping.png" width="100%"> <p>Fig. 4: Table about Shopping Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/qualitative-result-shopping.png" width="100%"> <p>Fig. 4.1: Qualitative results from Shopping Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/table2-pool-ball.png" width="75%"> <p>Fig. 5: Table about Pool Ball Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/qualitative-result-pool-ball.png" width="100%"> <p>Fig. 5.1: Qualitative results from Pool-Ball Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/success-rate-plot.png" width="100%"> <p>Fig. 6: Success Rate plot for baselines.</p> </div> <h5 id="visualize-heat-map">Visualize heat map</h5> <div align="center"> <img src="/assets/img/dream2real/visualize-renderings.png" width="100%"> <p>Fig. 7: Visualize renderings from Dream2Real.</p> </div> <h3 id="conclusion--limitation">Conclusion &amp; Limitation</h3> <ul> <li>Conclusion <ul> <li>저자는 zero-shot manner로 2D VLMs로 3D object rearrangement task를 수행할 수 있는 점을 큰 contribution으로 삼습니다.</li> <li>추후 연구로는 iterative하게 score를 refine하는 방식도 될 수 있을 것이며, multistage-task에 대해서도 수행할 수 있을 것입니다.</li> </ul> </li> <li>Limitation <ul> <li>하나의 scene을 NeRF로 reconstruct 하는 것에 3~5분 정도 소요된다고 합니다.</li> <li>orientation을 discrete set에서 샘플하여, orientation이 더욱 복잡한 task에는 적용하기 어렵다고 합니다. (sampling high-resolution -&gt; high computation cost)</li> <li>CLIP에서 bag-of words behaviour의 현상이 일어납니다. 이는 Text에서 단어의 순서가 중요한 경우 입니다. <ul> <li>e.g. <code class="language-plaintext highlighter-rouge">“a fork to the left of a knife” often places the knife to the left of the fork instead."</code> </li> </ul> </li> </ul> </li> </ul> <h3 id="thoughts">Thoughts:</h3> <ul> <li>spots 논문의 컨셉과 유사한 것 같아 읽었습니다. 해당 논문에서는 NeRF로 전체 workspace와 target object를 복원하고, target object의 orientation도 고려해가며 physics check를 합니다.</li> <li>다만 NeRF 기반의 방식이라 하나의 씬에 3~5분 정도 걸린다는 한계가 있다고 저자가 밝힙니다. real-to-sim 측면을 조금 강조하려면 해당 연구에서 한 방식대로 3d mesh를 만들어 내는 것이 좋을 것 같습니다.</li> <li>그리고 해당 논문에서는 score heatmap을 정의해 physics-check + similarity check를 통해 best-pose 하나만을 찾습니다. CLIP으로 이미지와 desired goal-pose(caption) 간의 similarity로 best pose를 찾습니다.</li> <li>여기서 goal-caption은 object pose에 대한 내용만을 이루며, 저희가 spots에서 수행했던 semantic reasoning과는 다르다고 느꼈습니다.</li> <li>spots 논문에 이어서, physics check를 여러 robot action에 대해서 수행하는 방향으로 확장하면 재밌을 것 같습니다.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/6dof-graspnet/">[paper-review] 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/stow/">[paper-review] Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/apriltag-pose-estimation/">[구현] AprilTag Pose Estimation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/rba/">[paper-review] Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/exo-pbl/">[paper-review] User preference optimization for control of ankle exoskeletons using sample efficient active learning</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joonhyung-lee/joonhyung-lee.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Joel Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
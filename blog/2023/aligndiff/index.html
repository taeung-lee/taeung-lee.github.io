<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [paper-review] AlignDiff: Aligning Diverse Human Preferences via Behavior-customisable Diffusion Model | Joel Lee </title> <meta name="author" content="Joel Lee"> <meta name="description" content="paper review about AlignDiff"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joonhyung-lee.github.io//blog/2023/aligndiff/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Joel Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">miscellaneous </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/gallery/">gallery</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[paper-review] AlignDiff: Aligning Diverse Human Preferences via Behavior-customisable Diffusion Model</h1> <p class="post-meta"> October 17, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion</a>   <a href="/blog/tag/preference-based-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Preference-Based Learning</a>   <a href="/blog/tag/arxiv"> <i class="fa-solid fa-hashtag fa-sm"></i> Arxiv</a>   <a href="/blog/tag/2023"> <i class="fa-solid fa-hashtag fa-sm"></i> 2023</a>     ·   <a href="/blog/category/paper-review"> <i class="fa-solid fa-tag fa-sm"></i> paper-review</a>   <a href="/blog/category/paper-review-pbl"> <i class="fa-solid fa-tag fa-sm"></i> paper-review/PBL</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>Arxiv 2023. [<a href="https://arxiv.org/pdf/2310.02054.pdf" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://aligndiff.github.io/" rel="external nofollow noopener" target="_blank">Project Page</a>] [<a href="https://github.com/aligndiff/aligndiff.github.io" rel="external nofollow noopener" target="_blank">Github</a>]</p> <p>Zibin Dong<sup>1</sup>, Yifu Yuan<sup>1</sup>, Jianye Hao†<sup>1</sup>, Fei Ni<sup>1</sup>, Yao Mu<sup>3</sup>, Yan Zheng<sup>1</sup>, Yujing Hu<sup>2</sup>, Tangjie Lv<sup>2</sup> , Changjie Fan<sup>2</sup>, Zhipeng Hu<sup>2</sup> <sup>1</sup>College of Intelligence and Computing, Tianjin University, <sup>2</sup>Fuxi AI Lab, Netease, Inc., Hangzhou, China, <sup>3</sup>The University of Hong Kong</p> <p>3 Oct 2023</p> </blockquote> <h2 id="한-문장-요약">한 문장 요약</h2> <p>요약: Human preference를 RLHF로 quantify 하였고, 이를 diffusion 모델로 잘 포착해보자.</p> <p>Keyword: <strong>Abstractness</strong>, <strong>Mutability</strong></p> <h3 id="abstract">Abstract</h3> <p>Human preference를 RL에서 수행해왔지만, Abstractness, Mutability를 해석하는 것이 어려웠음.</p> <ol> <li>저자는 Abstractness를 RLHF로 해결하고, <ul> <li>Multi-Prospective Human Feedback Dataset을 crowdsourcing으로 구함. 이 데이터셋을 기반으로 Attribute strength를 예측하는 모델을 학습함.</li> </ul> </li> <li>Mutability를 Guide-Diffusion model로 해결하려 함. <ul> <li>1.에서 학습을 통해 구한 Attribute를 condition으로 한 diffusion model을 선보임.</li> </ul> </li> </ol> <h3 id="용어-설명">용어 설명</h3> <ul> <li>Abstractness: Relative behavioral attributes 논문에서 소개된 내용. Human preference의 추상적인 의미를 뜻하며, 이를 attribute라는 것으로 정량화 함. 좀 더 자세히 설명하면, Trajectory의 sequence를 기반으로 Behavior Attribute를 정량화 함 <strong>(=relative attribute strength).</strong> <ul> <li>여기서 Behavior Attribute는 PBL에서 수행되던 feature vector와 유사한 의미를 가짐. <ul> <li>ex) Speed, Jump height, Torso height, Stride length</li> </ul> </li> <li>In the paper <a href="https://arxiv.org/pdf/2210.15906.pdf" rel="external nofollow noopener" target="_blank">Relative behavioral attributes</a> (ICLR, 2023); <blockquote> <p>We introduce the notion of <strong>Relative Behavioral Attributes (RBA)</strong> to capture <strong>the relative strength of some properties’ presence in the agent’s behavior.</strong> We aim to learn an attribute-parameterized reward function that can encode complex task knowledge while allowing end users to freely increase or decrease the strength of certain behavioral attributes such as “step size” and “movement softness”</p> </blockquote> </li> </ul> </li> <li>Mutability: preference가 개개인별로 다르며, 시간에 따라 변화한다는 것을 의미함. <ul> <li>In the paper <a href="https://arxiv.org/pdf/2301.10677.pdf" rel="external nofollow noopener" target="_blank">IMITATING HUMAN BEHAVIOUR WITH DIFFUSION MODELS</a> (ICLR, 2023); <ul> <li>CLASSIFIER-FREE GUIDANCE에 대해 varying guidance strengths에 따른 분포의 변화를 보여주는 논문. <ul> <li>Diffusion-X / Diffusion-KDE 모델을 소개함.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="introduction">Introduction</h3> <ul> <li> <strong>Abstractness:</strong> Relative Behavioral Attribute (RBA)를 통해 attribute(=preference feature)에 대해 추정할 수 있지만, 이는 single-step state-action에 대한 근시안적인(myopic) 한계를 보여준다. <ul> <li> <p>Trajectory 전반적으로 잘 해석할 수 있어야 한다.</p> <p><strong>→ 1) RLHF로 Dataset을 relabelling 하였으며,</strong></p> <p><strong>→ 2) Transformer-based attribute strength model을 제안함.</strong></p> </li> </ul> </li> <li> <strong>Mutability:</strong> 사용자의 preference를 잘 포착하더라도, 이러한 preference는 일시적이지 않다. <a href="https://arxiv.org/pdf/2210.15906.pdf" rel="external nofollow noopener" target="_blank">기존의 방법</a>에 대해서는 매번 Reward fine-tuning을 수행해주어야 하는 치명적인 단점이 존재한다. <ul> <li>Retraining을 없애자. → <strong>Diffusion model</strong> </li> </ul> </li> <li>Agent의 preference matching / switching / covering capability를 평가하기 위한 Metric을 제시함. <ul> <li>Mean Absolute Error(<strong>MAE</strong>) between the evaluated and target relative strength</li> </ul> </li> </ul> <h3 id="related-works">Related Works</h3> <ul> <li>RLHF <ul> <li>기존의 RLHF는 single objective를 최적화 하는 것에 초점이 이루어짐.</li> <li>RBA는 attribute-conditioned reward model로 human preference를 distill 하는 시도를 보임. <ul> <li>다만 이는 Single-step에 대한 것이므로 전반적인 trajectory에 대해서 충분히 고려하지 못함.</li> <li>새로운 preference에 대해 매번 retraining이 필수적임.</li> </ul> </li> </ul> </li> <li>Diffusion Models for Decision-Making <ul> <li>Reward(or auxiliary information) conditioned 하여 decision trajectory를 생성하는 시도가 이루어졌음.</li> <li>그러나 이는 학습된 분포 내에서 한정적인 부분에 대해서만 생성이 가능했음. <ul> <li>학습된 분포에 대해 전반적으로 잘 활용할 수 있게 해보자.</li> </ul> </li> </ul> </li> </ul> <h3 id="preliminaries">Preliminaries</h3> <ul> <li>Problem Setup <ul> <li> \[\mathcal{M}=&lt;\mathrm{S,~A,~P~,\alpha}&gt;\] <ul> <li>\(\mathrm{S}:\) Set of states</li> <li>\(\mathrm{A}\): Set of actions</li> <li>\(\mathrm{P}: \mathrm{S\times A\times S}~\rightarrow~[0,1]\): Transition function</li> <li>\(\mathrm{\alpha}=\{\alpha_{1},\cdots,\alpha_{k}\}\): a set of k predefined attributes used to characterize the agent’s behavior <ul> <li>기 정의된 attribute에 대한 내용: ex) Speed, Jump height, Torso height, Stride length</li> </ul> </li> <li>\(\mathrm{\tau}^{l}=\{s_{0},\cdots,s_{l-1} \}\): length l의 길이만큼 state \(s\)를 갖는 궤적에 대한 표현.</li> <li>\(\zeta^{\alpha}(\tau^{l})=\boldsymbol{v}^{\alpha}=[v^{\alpha_{1}},\cdots,v^{\alpha_{k}}]\in [0,1]^{k}\): 궤적을 입력으로 받아, 해당 궤적이 각 attribute에 대해 얼마 만큼의 값을 가지는지 mapping 해주는 함수. <ul> <li>\(v^{\alpha_{i}}\)의 값이 클 수록 해당 Attribute \(\alpha_{i}\)를 잘 표방한다는 것이며, 작을 수록 그 반대의 의미.</li> </ul> </li> </ul> </li> <li>저자는 Human preference를 \((\boldsymbol{v^{\alpha}}_{\text{targ}},\boldsymbol{m}^{\alpha})\)의 쌍으로 정의함. <ul> <li>\(\boldsymbol{v^{\alpha}}_{\text{targ}}\): relative strength를 의미함.</li> <li>\(\boldsymbol{m}^{\alpha}\in\{0,1\}^{k}\): binary masking 값을 의미함. 즉, 해당하는 interest of attribute를 뜻함.</li> </ul> </li> <li>Objective <ul> <li> <table> <tbody> <tr> <td>find a policy $$a=\pi(\mathrm{s}</td> <td>\boldsymbol{v^{\alpha}}_{\text{targ}},\boldsymbol{m^{\alpha}})$$ that minimizes the L1 norm</td> </tr> </tbody> </table> </li> </ul> \[||(\boldsymbol{v^{\alpha}}_{\text{targ}}-\zeta^{\boldsymbol{\alpha}}(\mathop{\mathbb{E}_{\pi}}[\tau^{l}]))~\circ~\boldsymbol{m^{\alpha}}||_{1}\] </li> </ul> </li> <li>PbRL <ul> <li>Bradley-Terry objective에 대한 서술.</li> </ul> </li> <li>DDIM, Classifier-free Guidance (CFG) <ul> <li>DDIM, CFG에 대한 일반적인 서술.</li> </ul> </li> <li>한 줄 요약: <ul> <li>We learn human preferences from an unlabeled state-action dataset \(D=\{\tau\}\), which contains multiple behaviors.</li> </ul> </li> </ul> <h3 id="methodology">Methodology</h3> <div style="text-align:center"> <img src="/assets/img/aligndiff/aligndiff_method.png" alt="Alignment Method Image" width="100%"> <p>Overall framework of AlignDiff.</p> </div> <p>저자가 제안하는 방법론 AilgnDiff는 총 4개의 파트로 이루어짐.</p> <ol> <li>RLHF Dataset: multi-prospective human feedback through crowdsourcing. <ul> <li>\(D\): Dataset → \(D_{p}=\{(\tau_{1},\tau_{2},y_{\text{attr}})\}, y_{\text{attr}}=(y_{1},\cdots,y_{k})\)</li> </ul> </li> <li>Train an attribute strength model, which we then use to relabel the behavioral datasets. <ul> <li>\(D_{G}\): Annotated dataset</li> </ul> </li> <li>Train a diffusion model with an annotated dataset <ul> <li>which can understand and generate trajectories with various attributes.</li> </ul> </li> <li>AlignDiff for inference, aligning agent behaviors with human preferences at any time.</li> </ol> <p>각 파트를 정리하면,</p> <ol> <li>Multi-Perspective Human Feedback Collection <ul> <li>Trajectory dataset \(D\)로부터 2개의 pair 궤적 \(\{(\tau_{1}, \tau_{2})\}\)을 사람들에게 각 attribute에 대해서 어떠한 궤적이 더 적합한지 물어본다. <ul> <li>feedback dataset \(D_{p}=\{(\tau_{1},\tau_{2},y_{\text{attr}})\}\)을 얻는다. Attribute의 예시로는 \((\text{speed, stride, humanness})\) 등이 있다.</li> </ul> </li> </ul> </li> <li>Attribute Strength Model Training <ul> <li>저자는 Bradley-Terry objective를 수정하여 제시함. <ul> <li>기존의 수식에서 달라진 점은 reward(=feature) mapping function 뿐이다. <ul> <li>\(r(\xi(\tau_1))\) → \(\hat{\zeta}^{\boldsymbol{\alpha}}_{\theta,i}(\tau_1)\)</li> </ul> </li> </ul> </li> </ul> \[P^{\alpha_{i}}[\tau_{1}\succ\tau_{2}]=\frac{\exp \hat{\zeta}^{\boldsymbol{\alpha}}_{\theta,i}(\tau_1)}{\sum_{j\in\{1,2\}}\exp\hat{\zeta}^{\boldsymbol{\alpha}}_{\theta,i}(\tau_{j})}\] <ul> <li>해당 수식이 의미하는 바는, attribute \(\alpha_{i}\)를 기준으로 궤적 1이 더 적합한 경우에 대한 수식.</li> <li>Loss objective: <strong>Relative Attribute Strength Network(model)</strong> \(\begin{equation*} \mathcal{L}(\hat{\zeta}^{\boldsymbol{\alpha}}_{\theta}) = -\sum_{(\tau_{1},\tau_{2},y_{\text{attr}})\in D_p} \sum_{i=1}^{k} y_{i}(1)\log P^{\alpha_{i}}[\tau_{1} \succ \tau_{2}] + y_{i}(2)\log P^{\alpha_{i}}[\tau_{2} \succ \tau_{1}] \end{equation*}\) <ul> <li>저자도 언급하길, 이거는 단순히 Bradley-Terry objective에서의 Mapping 함수를 바꿔준 정도라고만 함.</li> <li>Introduction에서 언급한 single-step에 대해서만 추정하는 한계점을 \(\zeta\) 라는 mapping 함수를 통해 variable-length trajectory input에 대해서 적용할 수 있다는 점을 언급함. <ul> <li>이를 Transformer 구조로 학습하며, learnable embedding을 추가해 relative strength vector \(\boldsymbol{v^{\alpha}}\)를 학습하는 것을 목표로 함. (입력은 state-only 궤적을 받음.) <ul> <li>Transformer를 거친 후에, 마지막 layer에는 linear layer를 통해 \(\boldsymbol{v^{\alpha}}\)를 얻음.</li> </ul> </li> </ul> </li> <li>학습이 수행된 이후에는 \(\hat{\zeta}^{\boldsymbol{\alpha}}_{\theta,i}(\cdot)\)을 통해 annotated dataset \(D_{G}=\{(\tau^{H},\boldsymbol{v^{\alpha}})\}\)을 얻음. <ul> <li>주의할 점은, 여기서 저장되는 trajectory는 모두 고정된 length \(H\)를 가짐. 위 데이터셋 \(D_{G}\)을 통해 diffusion training이 이루어짐. Attribute-Conditioning을 위해 수행한 과정.</li> </ul> </li> </ul> </li> </ul> </li> <li>Diffusion Training <img src="/assets/img/aligndiff/aligndiff_architecture.png" alt="Diffusion Training Architecture"> <ul> <li>DDIM 구조에 condition으로 \(\boldsymbol{v^{\alpha}},\boldsymbol{m^{\alpha}}\)을 줌. <ul> <li>[Conditioned / Unconditioned] noise predictor \([\epsilon_{\phi}(\boldsymbol{x}_{t},\boldsymbol{v^{\alpha}},\boldsymbol{m^{\alpha}})\) / \(\epsilon_{\phi}(\boldsymbol{x}_{t})]\) - masking \(\boldsymbol{m^{\alpha}}\)가 conditioning을 관여하므로, 한 개의 network를 학습하면 된다.</li> </ul> </li> <li>Diffusion backbone으로는 U-Net 대신에 <strong>DiT</strong> 모델을 사용하였으며, 구조에는 일부 수정이 있음. <ul> <li> <a href="https://github.com/facebookresearch/DiT" rel="external nofollow noopener" target="_blank">Scalable Diffusion Models with Transformers (DiT)</a> <ul> <li>Transformer-based Backbone.</li> </ul> </li> </ul> </li> <li>이러한 condition을 잘 활용하기 위해 2개의 requirement가 있다고 함. <ol> <li>\(\boldsymbol{m^{\alpha}}\) should <strong>eliminate</strong> the influence of nonrequested attributes on the model while preserving the effect of <strong>the interested attributes</strong> <ul> <li>masking vector를 통해 attribute 별로 독립적으로 고려할 수 있게 하겠다의 의미?</li> </ul> </li> <li>\(\boldsymbol{v^{\alpha}}\) cannot be simply multiplied with \(\boldsymbol{m^{\alpha}}\) and fed into the network, as a value of \(0\) in \(\boldsymbol{v^{\alpha}}\) still carries specific meanings. <ul> <li>1의 속성으로 인해 masking vector와 단순 곱을 사용하게 되면 잘못된 의미가 된다?</li> </ul> </li> <li>이를 만족하기 위해 attribute-oriented encoder를 제안함. <ul> <li>\(\boldsymbol{v^{\alpha}}\)를 \(V\)개의 selectable token으로 표현해주기 위함.</li> </ul> \[v^{\alpha_{i}}_{d}=[\text{clip}(v^{\alpha_{i}},0,1-\delta)~\cdot~V]+(i-1)V,~i=i,\cdots,k\] <ul> <li>\(\delta\): small slack variable. <ul> <li>This ensures that each of the V possible cases for each attribute is assigned a unique token.</li> <li>그렇게 임베딩을 거친 \(\boldsymbol{v^{\alpha}}\)는 both attribute category and strength의 정보를 포함하게 된다.</li> </ul> </li> <li>Loss objective: <strong>Noise predictor loss of Diffusion mode</strong> \(\begin{equation*} \mathcal{L}(\phi) = \mathop{\mathbb{E}}_{(\boldsymbol{x}_{0},\boldsymbol{v}^{\alpha})\sim\mathcal{D}_{G},~t\sim\text{Uniform}(T),~\epsilon\sim\mathcal{N}(0,I),~\boldsymbol{m}^{\alpha}\sim\mathcal{B}(k,p)} \left\| \epsilon - \epsilon_{\phi}(\boldsymbol{x}_{t}, t, \boldsymbol{v}^{\alpha}, \boldsymbol{m}^{\alpha}) \right\|_{2}^{2} \end{equation*}\)</li> </ul> </li> </ol> </li> </ul> </li> <li>AlginDiff Inference <ul> <li>앞선 과정에서 구한 attribute strength model \(\hat{\zeta}^{\boldsymbol{\alpha}}_{\theta}\)와 noise predictor \(\epsilon_{\phi}\)를 통해 AlginDiff를 소개한다. 저자는 DDIM 모델을 사용했으며, Inpainting 방식처럼 \(\kappa\)를 length \(S\) 내에서 반복적으로 candidate trajectory를 생성했다. \(\begin{equation*} \boldsymbol{x}_{\kappa_{i-1}} = \sqrt{\xi_{\kappa_{i-1}}} \left( \frac{\boldsymbol{x}_{\kappa_{i}} - \sqrt{1 - \xi_{\kappa_{i}}} \tilde{\epsilon}_{\phi}(\boldsymbol{x}_{\kappa_{i}})}{\sqrt{\xi_{\kappa_{i}}}} \right) + \sqrt{1 - \xi_{\kappa_{i-1}} - \sigma^{2}}_{\kappa_{i-1}} \tilde{\epsilon}_{\phi}(\boldsymbol{x}_{\kappa_{i}}) + \sigma_{\kappa_{i}} \epsilon_{\kappa_{i}} \end{equation*}\)</li> </ul> </li> </ol> <p>그렇게 최종 objective equation은 아래와 같다. \(\mathcal{J}(\tau)=||(\boldsymbol{v^{\alpha}}-\hat\zeta^{\alpha}_{\theta}(\tau))\circ\boldsymbol{m^{\alpha}}||^{2}_{2}\)</p> <blockquote> <p>Each \(\tau\) in the candidate trajectories satisfies human preference \((\boldsymbol{v^{\alpha}},\boldsymbol{m^{\alpha}})\) a priori. Then we utilize \(\hat{\zeta}^{\alpha}_{\theta}\) to criticize and select the most aligned one to maximize the following objective:</p> </blockquote> <div align="center"> <img src="/assets/img/aligndiff/aligndiff_inference.png" width="100%"> <p>Inference Flowchart.</p> </div> <p>실제 Inference가 수행될 때에는 language command가 주어지고, 이를 Sentence-BERT에 태워서 corpus와 mapping을 거치게 된다. → 의도한 attribute와 가장 유사한 것으로 mapping 됨.</p> <h3 id="experiments">Experiments</h3> <p>아래의 4개의 질문에 대한 답을 하기 위해 실험을 설계함.</p> <ul> <li>Matching (RQ1): Can AlignDiff better align with human preferences compared to other baselines?</li> <li>Switching (RQ2): Can AlignDiff quickly and accurately switch between different behaviors?</li> <li>Covering (RQ3): Can AlignDiff cover diverse behavioral distributions in the dataset?</li> <li>Robustness (RQ4): Can AlignDiff exhibit robustness to noisy datasets and limited feedback?</li> </ul> <div align="center"> <img src="/assets/img/aligndiff/aligndiff_attribute.png" width="75%"> <p>실험에서 사용된 Attribute.</p> </div> <ul> <li> <strong>Baselines</strong> <ul> <li>Goal-conditioned behavior clone (GC): <strong>RvS (RL VIA SUPERVISED LEARNING)</strong> <ul> <li> <a href="https://openreview.net/pdf?id=S874XAIpkR-" rel="external nofollow noopener" target="_blank">RvS: What is essential for offline RL via supervised learning?</a> (ICLR, 2022) <ul> <li>conditioning on <strong>reward or goal /</strong> Simple 2-Layered MLP architecture</li> </ul> </li> </ul> </li> <li>Sequence modeling (SM): <strong>Decision Transformer (DT)</strong> <ul> <li> <a href="https://sites.google.com/berkeley.edu/decision-transformer" rel="external nofollow noopener" target="_blank">Decision Transformer: Reinforcement Learning via Sequence Modeling</a>(NeurIPS, 2021)</li> </ul> </li> <li>TD Learning (TDL): <strong>TD3BC</strong> <ul> <li> <a href="https://proceedings.neurips.cc/paper/2021/hash/a8166da05c5a094f7dc03724b41886e5-Abstract.html" rel="external nofollow noopener" target="_blank">A minimalist approach to offline reinforcement learning.</a> (NeurIPS, 2021) <ul> <li>TD3 policy update eqn.에 behavior cloning regularization term을 추가함.</li> <li>dataset의 state feature에 대해 normalize를 수행함.</li> <li>저자가 소개하길, 이는 RBA의 upgrade 버전이라고 함.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p><img src="/assets/img/aligndiff/aligndiff_figure4.png" alt="Diffusion Training Architecture" style="width: 100%;"> <img src="/assets/img/aligndiff/aligndiff_table2.png" alt="Diffusion Training Architecture" style="width: 100%;"></p> <ul> <li>Evaluation by attribute strength model: <ul> <li>The mean absolute error(MAE) between the evaluated and target relative strength를 metric으로 삼음.</li> </ul> </li> <li>Track the changing target attributes: <img src="/assets/img/aligndiff/aligndiff_figure6.png" alt="Diffusion Training Architecture" style="width: 100%;"> <ul> <li>총 800번의 simulation step 중, target attribute \(\boldsymbol{v^{\alpha}}_{\text{targ}}\)을 \((0,200,400,600)\)에서 수정을 함. <ul> <li>빠르게 tracking 한다는 결과를 보여줌.</li> </ul> </li> </ul> </li> <li>Covering attribute strength distribution: <img src="/assets/img/aligndiff/aligndiff_figure5.png" alt="Diffusion Training Architecture" style="width: 100%;"> <ul> <li>AlignDiff 을 통해 얻은 network \(\hat\zeta^{\alpha}_{\theta}(\tau)\)가 GT와 유사하게 값을 잘 추정해낸다는 것을 보여줌.</li> </ul> </li> </ul> <h3 id="conclusion">Conclusion</h3> <ul> <li>Abstractness <ul> <li>RLHF로 Human preference에 대한 dataset을 통해 preference를 quantify.</li> </ul> </li> <li>Mutability <ul> <li>변화하는 preference를 conditioned-diffusion model로 해결함.</li> </ul> </li> </ul> <h3 id="appendix">Appendix</h3> <ul> <li>Implementation details.</li> <li>Dataset details.</li> </ul> <h3 id="thoughts">Thoughts</h3> <ul> <li>Abstractness와 Mutability라는 용어를 처음으로 제시했으며, 각각에 해당하는 한계점을 잘 녹여냈음. <ul> <li>Abstractness를 위해 RLHF labeling을 거치며, appendix에서 이 data에 대한 reliableness도 보장하였음. <ul> <li>Attribute 요소를 포괄하는 상위 개념의 Persona?로 정의해서 표현해보는 방향도 재밌을 것 같다. (교수님 취향)</li> </ul> </li> <li>Mutability라는 용어로 preference의 변화를 표현했으며, 실험 결과도 이를 잘 보여주었음. 다만 Figure 6.에서 보듯이, oscilation 이 꽤 심하게 있는 것 같음. <ul> <li>해당 용어는 Time-varying 뿐만 아니라, 사용자에 따라 달라지는 preference도 포함한 개념.</li> <li><strong>Agent 끼리의 Attribute를 transfer 해주는 것도 고려하면 재밌을 것 같음.</strong></li> </ul> </li> <li>Figure 5.를 통해 동일한 attribute 내에서는 interpolation이 가능하다고 표현했으며 잘 보여줌. <ul> <li> <strong>동일 Agent 혹은 서로 다른 Agent의 서로 다른 Attribute 간의 interpolation도 가능하면 매우 파격적일 것 같음.</strong> <ul> <li>ex) skill composition or preference composition 느낌으로?</li> </ul> </li> </ul> </li> <li>Diffusion model을 사용하다보니 Inference에 시간이 오래 걸린다고 한다. 구체적인 inference 시간도 같이 알려줬으면 좋았을 것 같다. (Baseline과 비교하여)</li> </ul> </li> <li>Dataset <ul> <li>실험에서 사용한 Attribute가 조금 단순하다고 여겨짐. 다만, 논문의 예시들은 시각적으로 효과적인 차이를 잘 보여준 것 같음. <strong>이러한 Attribute를 더 다양하고 적은 label 데이터로 수행할 수 있으면 좋을 것 같음. (혹은 자동 labelling)</strong> </li> </ul> </li> </ul> <h3 id="reference-papers">Reference papers</h3> <ul> <li>Aligning artificial intelligence with human values: reflections from a phenomenological perspective (SpringerLink, 2021) <ul> <li>한 문장 요약: <blockquote> <p>This paper contributes the unique knowledge of phenomenological theories to the discourse on AI alignment with human values.</p> </blockquote> </li> <li>서술적인 내용들로만 이루어져 있음.</li> </ul> </li> <li> <strong>Relative behavioral attributes:</strong> Filling the gap between symbolic goal specification and reward learning from human preferences. <strong>(ICLR, 2023)</strong> <ul> <li>한 문장 요약: <blockquote> <p>In most cases, humans are aware of how the agent should change its behavior along meaningful axes to fulfill their underlying purpose, even if they are not able to fully specify task objectives symbolically: <strong>allows the users to tweak the agent behavior through symbolic concepts.</strong></p> </blockquote> </li> <li>Attribute를 표현한 논문 <ul> <li><a href="https://openreview.net/forum?id=ULCjqBDpZa" rel="external nofollow noopener" target="_blank">Open Review</a></li> <li><a href="https://github.com/GuanSuns/Relative-Behavioral-Attributes-ICLR-23" rel="external nofollow noopener" target="_blank">Code</a></li> </ul> </li> </ul> </li> <li> <strong>Inverse Reward Design (읽어볼 것 / NIPS, 2017)</strong> <ul> <li>한 문장 요약: <blockquote> <p>We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs.</p> </blockquote> </li> </ul> </li> <li>Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity (PMLR, 2022) <ul> <li>한 문장 요약: <blockquote> <p>Symbolic model과 MDP를 합쳐서 skill learning을 수행해보자. (Approximate Symbolic-Model Guided Reinforcement Learning)</p> </blockquote> </li> </ul> </li> <li>Imitating Human Behaviour with Diffusion Models <strong>(2023, ICLR)</strong> <ul> <li>한 문장 요약: <blockquote> <p>We then propose that diffusion models are an excellent fit for imitating human behavior. (This paper studies their application as observation-to-action models for imitating human behavior in sequential environments.) - Diffusion + KDE 방법론: Diffusion 모델로 여러 샘플을 얻고, 그에 해당하는 KDE 구조를 사용함.</p> </blockquote> <ul> <li>observation-to-action diffusion models 을 분석하고 소개함.</li> <li> <img src="/assets/img/diffusion-bc/diffusion-bc-framework.png" alt="Diffusion Training Architecture" style="width: 100%;"> <blockquote> <p>‘Diffusion-X’ and ‘Diffusion-KDE’ as variants of Diffusion BC, that mirror this screening process by encouraging higher-likelihood actions during sampling. For both methods, the training procedure is unchanged (only the conditional version of the model is required)</p> </blockquote> <ul> <li><a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffusion-bc/" rel="external nofollow noopener" target="_blank">[논문리뷰] Imitating Human Behaviour with Diffusion Models</a></li> </ul> </li> </ul> </li> </ul> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/rba/">[paper-review] Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cpl/">[paper-review] Contrastive Prefence Learning: Learning from Human Feedback without RL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/6dof-graspnet/">[paper-review] 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/vq/">[study] Vector Quantization</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/promptablebehaviors/">[paper-review] Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joonhyung-lee/joonhyung-lee.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Joel Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
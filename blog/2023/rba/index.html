<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [paper-review] Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences | Joel Lee </title> <meta name="author" content="Joel Lee"> <meta name="description" content="paper review about RBA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joonhyung-lee.github.io//blog/2023/rba/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Joel Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">miscellaneous </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/gallery/">gallery</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[paper-review] Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences</h1> <p class="post-meta"> October 23, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/preference-based-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Preference-Based Learning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/tag/iclr"> <i class="fa-solid fa-hashtag fa-sm"></i> ICLR</a>   <a href="/blog/tag/2023"> <i class="fa-solid fa-hashtag fa-sm"></i> 2023</a>     ·   <a href="/blog/category/paper-review"> <i class="fa-solid fa-tag fa-sm"></i> paper-review</a>   <a href="/blog/category/paper-review-pbl"> <i class="fa-solid fa-tag fa-sm"></i> paper-review/PBL</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>ICLR 2023. [<a href="https://arxiv.org/pdf/2210.15906.pdf" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://guansuns.github.io/pages/rba/" rel="external nofollow noopener" target="_blank">Project Page</a>] [<a href="https://github.com/GuanSuns/Relative-Behavioral-Attributes-ICLR-23" rel="external nofollow noopener" target="_blank">Github</a>]</p> <p>Lin Guan, Karthik Valmeekam, Subbarao Kambhampati School of Computing &amp; AI Arizona State University Tempe, AZ 85281, USA</p> <p>27 Feb 2023</p> </blockquote> <h2 id="한-문장-요약">한 문장 요약</h2> <p>요약: tacit behavior를 attribute라고 정의하고, trajectory 내에서 relative behavior attribute를 추정하자. 즉, Relative Behavioral Attributes(RBA)를 정의해, symbolic feedback으로부터 희망하는 agent behavior를 얻자.</p> <blockquote> <p>In most cases, humans are aware of how the agent should change its behavior along meaningful axes to fulfill their underlying purpose, even if they are not able to fully specify task objectives symbolically: <strong>allows the users to tweak the agent behavior through symbolic concepts.</strong></p> </blockquote> <p>Keyword: <strong>Relative Behavioral Attributes</strong> <br></p> <h3 id="introduction">Introduction</h3> <ul> <li>Agent의 Behavior를 정의하는 방식은 크게 2가지가 있다. <ol> <li>Reward function을 직접 지정해주는 경우. <ul> <li>직접 특정 behavior를 유도하게끔 하는 수식적인 reward function을 explicit하게 정하는 것이며, 논문에서는 이를 <code class="language-plaintext highlighter-rouge">symbolic reward machine</code> 이라고 표현함.</li> <li>하지만 exact description이 가능한 경우는 매우 한정적이며(=<code class="language-plaintext highlighter-rouge">domain specific</code>), <code class="language-plaintext highlighter-rouge">expert knowledge</code>를 요한다.</li> </ul> </li> <li>궤적으로부터 Reward function을 추정하는 경우. <ul> <li>기존의 PBL은 궤적에 대해 이러한 feedback(=binary comparison) 과정을 거쳐 reward function을 추정해왔으며, 이는 preference labelling에 대한 cost가 높았다.</li> <li>그리고 학습된 neural network는 typically inscrutable 하므로 실질적으로 reward model을 재사용할 수도 없으며, internal representation을 해석할 수도 없다.</li> </ul> </li> </ol> </li> <li>위 두 방식에서 수행된 behavior에 대한 표현력은 제한적이었으며, 이러한 한계점을 해결하기 위해 저자는 <code class="language-plaintext highlighter-rouge">attribute</code>를 정의하며 논문을 시작한다. <ul> <li>기존의 pbl에서 사용되던 <code class="language-plaintext highlighter-rouge">feature function</code> \(\phi (\xi)\) 와 유사하게, <code class="language-plaintext highlighter-rouge">attribute</code>라는 용어를 정의하며, 이는 behavior에 대해 symbolic concept을 가진다. (<code class="language-plaintext highlighter-rouge">e.g., increasing the softness or speed of agents’ movement</code>)</li> <li>기존에 computer vision에서도 <code class="language-plaintext highlighter-rouge">attribute</code>라는 용어가 사용되어왔지만, 이는 single image에서만 활용됐으며, agent의 궤적에 활용하는 것은 본 논문이 첫 시도임. <br> </li> </ul> </li> </ul> <h3 id="preliminaries">Preliminaries</h3> <div style="text-align:center"> <img src="/assets/img/rba/rba_vis_attribute_appendix.png" alt="RBA-overview" width="100%"> <p>Fig. 1: Visualization of Behavior Attributes.</p> </div> <ul> <li>Attribute: agent’s behavior features(=descriptions) <ul> <li>총 4개의 Task에 대해서 9개의 Attribute를 정의했음. <code class="language-plaintext highlighter-rouge">Appendix A.1</code> 참조.</li> </ul> </li> <li>\(\alpha \in \mathcal{A}\): attribute의 종류, <code class="language-plaintext highlighter-rouge">e.g., softness of movement, step(=stride) size</code> </li> <li>\(h\): <code class="language-plaintext highlighter-rouge">attribute</code>를 binary masking 해주는 역할.</li> <li>\(v^{ {\alpha}_{i} }_{t}\): target attribute-score vector, \(\alpha_i\)에 해당하는 target scalar value</li> <li>\(\mathcal{D}\): State-only Trajectory pool</li> <li>\(\zeta_{\sigma}(\tau ', \alpha_{i}) \simeq \boldsymbol{v}^{\alpha_{i}}_{t}\): target attribute score \(\boldsymbol{v}\)에 대한 mapping function \(\zeta\) <br> </li> </ul> <h3 id="methodology">Methodology</h3> <div style="text-align:center"> <img src="/assets/img/rba/rba_overview.png" alt="RBA-overview" width="100%"> <p>Fig. 2: Overall framework of RBA.</p> </div> <blockquote> <p>Our goal is to construct a reward function that allows end users to iteratively adjust attribute strengths presented in the agent’s behavior.</p> </blockquote> <p><br></p> <h4 id="section-41-personalizing-agent-behavior-via-relative-behavior-attributes">Section 4.1 Personalizing Agent Behavior Via Relative Behavior Attributes</h4> <ul> <li>Trajectory sequence에서 relative strength of some property(=<code class="language-plaintext highlighter-rouge">attribute</code>)를 찾으려는 것이 목적이며, 이를 2개의 phase로 구분한다. <ol> <li>Leraning an attribute parameterized reward function (no interaction with the end user). <blockquote> <p>To learn a reward function that internally learns a family of rewards that correspond to behaviors with diverse attribute strength</p> </blockquote> <ul> <li>given a subset of labelled trajectories from an offline behavior dataset \(\mathcal{D}\). <ul> <li>여기서 Training label은 expert engineer 혹은 developer가 제공하는 것으로 한정한다.</li> </ul> </li> </ul> </li> <li>Interaction with the end user (Supporting end users in the loop). <blockquote> <p>Once an attribute parameterized reward function is learned, any incoming users can leverage it to personalize the agent behavior through multiple rounds of query</p> </blockquote> <ul> <li>set of attribute pairs \(\{ (\alpha, h)\}\) 를 기준으로 user가 feedback을 준다. <blockquote> <p>whether the current behavior is desirable.</p> </blockquote> <ul> <li>Attribute representation은 2개로 정의했으며, 1) Index of \(\alpha\), 2) a Natural-Language Description of \(\alpha\).</li> </ul> </li> </ul> </li> </ol> </li> <li>저자는 총 2개의 method 를 제시함. <strong>(1) RBA-Global / (2) RBA-Local</strong> <br> </li> </ul> <h4 id="section-42-modeling-behavior-attributes-by-establishing-global-rankings-rba-global">Section 4.2 Modeling Behavior Attributes by Establishing Global Rankings: RBA-Global</h4> <ul> <li>RBA-Global은 2개의 과정으로 이루어진다. <ol> <li>Learn attribute strength estimator \(\zeta_{\sigma}\) <ul> <li>기존의 Bradley-Terry model에서 exponential term이 zeta 함수로 변경되었으며, 궤적과 attribute가 주어졌을 때 그에 상응하는 strength 값으로 attribute mapping function zeta를 학습해주는 과정임.</li> <li>즉, attribute에 대한 ranking을 학습하는 과정임. <blockquote> <p>establishing a global ranking among all possible behaviors according to any given attribute \(\alpha\). Orderings according to different attributes \(\{ (\tau^{0} \succ \tau^{1} \succ \cdots \tau^{N} | \alpha)\}\) or a set of ranked trajectory pairs \(\mathcal{D}_{l}=\{(\tau^{i} \succ \tau^{j} | \alpha)\}\)</p> </blockquote> </li> <li>저자는 modified state-only version of Bradley-Terry model을 제안한다.: <blockquote> <p>Rather than assuming that the ranking is governed by the latent user preferences, We assume the ranking is determined by the given attribute \(\alpha\):</p> </blockquote> </li> </ul> \[\begin{equation*} \mathrm{P}_{\sigma}[\tau^{1} \succ \tau^{2} | \alpha] = \frac{\exp \sum_{t} f_{\sigma}([s^{1}_{t}, e_{\alpha}])}{\sum_{i \in \{ 1,2 \}}\exp \sum_{t} f_{\sigma}([s^{i}_{t}, e_{\alpha}] )} \end{equation*}\] <ul> <li>where \(f_{\sigma}\) is an attribute conditioned ranking function with parameters \(\sigma\), \([\cdot, \cdot]\) is the vector concatenation operation, and \(e_{\alpha}\) is the embedding of attribute \(\alpha\).</li> <li>\(e_{\alpha}\) can either be a one-hot vector or a sentence embedding generated by any pretrained natural language sentence encoder like \(\text{Sentence-BERT}\).</li> </ul> </li> <li> <table> <tbody> <tr> <td>Learn dense reward function $$r_{\theta}(s</td> <td>v_t= \left&lt; v^{\alpha_{1}}<em>{t}, \cdots, v^{\alpha</em>{k}}_{t} \right&gt;)$$</td> </tr> </tbody> </table> <ul> <li>trajectory pool에서 3개의 궤적 \(\tau_0, \tau_1, \tau_2\)을 sample하고, \(\tau_0\).을 target behavior로 간주하여 나머지 두 궤적 중 어떠한 것이 더 적합한지 cumulative reward를 통해 판단하게 되며, reward function을 학습하게 된다.</li> <li>위 과정에서는 human labelling 과정이 필요없다. zeta function이 자동으로 attribute labelling을 수행해주기 때문에.</li> </ul> </li> </ol> </li> </ul> <p><br></p> <h4 id="section-43-modeling-behavior-attributes-by-capturing-minimally-viable-local-changes-rba-local">Section 4.3 Modeling Behavior Attributes by Capturing Minimally Viable Local Changes: RBA-Local</h4> <ul> <li>RBA-Global에 대해서 확장된 개념이며, 보다 많은 수의 attribute가 있을 때에 효율적으로 학습하기 위해 본 방법론으로 확장했다고 주장한다.</li> <li>여기서는 \(\zeta\) function을 학습하지 않고, 곧바로 reward function \(r\) 을 학습한다. <blockquote> <p>Our goal is to construct a reward function that gives higher cumulative rewards to trajectories that have some minimal but noticeable change in \(\alpha\) in the direction specified by \(h\) while keeping other unmentioned attributes unchanged. We refer to such minimal but noticeable changes as minimally viable local changes, and the queried trajectory \(\tau_{c}\) as the anchor trajectory.</p> </blockquote> </li> <li> <table> <tbody> <tr> <td>$$r_{\theta}(\cdot</td> <td>\alpha, h, \tau_{c})\(를 학습하는 것이며, 이는 minimal but noticeable change in\)\alpha$$ 를 포착하는 것이 목적이다.</td> </tr> </tbody> </table> </li> <li>여기서 Dataset에서 가져오는 trajectory pair의 구조도 조금 달라진다; \(\mathcal{D}_{l}=\{ (\tau_{c}, \tau_{t}, \alpha, h)\}\) <ul> <li>\(\tau_{c}\): anchor trajectory</li> <li>\(\tau_{t}\): a trajectory that reflects some minimally viable local changes to the anchor trajectory \(\tau_{c}\)</li> </ul> </li> <li>이에 대해 queried trajectory \(\tau_{c}\) 를 anchor trajectory로 정의하며, 해당 궤적의 attribute \(\alpha\)를 기준으로 negative sample과 비교하여 trajectory \(\tau_{t}\)와 PBL을 수행해주는 과정이다. \(\begin{equation*} \mathrm{P}_{\sigma}[\tau^{t} \succ \tau^{n} | \alpha, h, \tau_{c}] = \frac{\exp \sum_{s \in \tau_{t}} r_{\theta}([s, e_{\alpha}, h, \phi(\tau_{c})])}{\exp \sum_{s \in \tau_{n}} r_{\theta}([s, e_{\alpha}, h, \phi(\tau_{c})] ) + \exp \sum_{s \in \tau_{t}} r_{\theta}([s, e_{\alpha}, h, \phi(\tau_{c})] )} \end{equation*}\) <ul> <li>where \(\tau_{n}\) is a negative sample, \([\cdot]\) is the vector concatenation operation, and \(\phi(\tau_{c})\) is a sequence encoder that encodes the anchor trajectory \(\tau_{c}\) to a compact latent representation. Note that \(\phi(\cdot)\) is a sub-module of \(r_{\theta}\) and it’s jointly optimized with \(r_{\theta}\).</li> <li>\(e_{\alpha}\) can either be a one-hot vector or a sentence embedding generated by any pretrained natural language sentence encoder like \(\text{Sentence-BERT}\). <ul> <li>저자는 이러한 구조가 <a href="https://arxiv.org/abs/2206.13499" rel="external nofollow noopener" target="_blank">Prompt-DT</a> 논문과 유사하다고 밝히며, 저자가 주장하는 차이점은 다음과 같다. <ul> <li>RBA method는 단순히 prompt의 behavior를 모방하는 것에 그치지 않고, prompt 내용(=<code class="language-plaintext highlighter-rouge">attribute</code>)을 수정하는 것을 학습한다고 한다.</li> </ul> </li> </ul> </li> <li>다만 이 방법론은 특정 attribute의 local change를 고려하므로 search space가 복잡해지며, 아무런 궤적을 막 가져오면 안 되고 local change가 잘 반영된 궤적을 가져와야 한다는 단점이 있다.</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/rba/rba_interaction_process.png" width="100%"> <p>Fig. 3: Interaction Visualization</p> </div> <ul> <li> <strong>Baselines</strong> <ul> <li>PbRL in (<a href="https://arxiv.org/abs/1706.03741" rel="external nofollow noopener" target="_blank">Christiano et al., 2017</a>; <a href="https://arxiv.org/abs/2106.05091" rel="external nofollow noopener" target="_blank">Lee et al., 2021</a>) <ul> <li>PbRL</li> <li>PEBBLE</li> </ul> </li> </ul> </li> <li>Results on RBA-Global:</li> </ul> <div align="center"> <img src="/assets/img/rba/rba_performance_global.png" width="75%"> <p>Fig. 4: Performance of RBA-Global</p> </div> <ul> <li>Results on RBA-Local:</li> </ul> <div align="center"> <img src="/assets/img/rba/rba_performance_local.png" width="75%"> <p>Fig. 5: Performance of RBA-Local</p> </div> <h3 id="appendix">Appendix</h3> <ul> <li>Behavior Attribute details.</li> <li>Implementation details.</li> <li>Experiment details.</li> </ul> <h3 id="thoughts">Thoughts</h3> <ul> <li>Behavior PBL을 처음으로 제시한 논문인 것 같습니다. 본 논문에서 언급한 것처럼, <code class="language-plaintext highlighter-rouge">attribute</code>라고 정의해, 말로 설명하기 어려운 <code class="language-plaintext highlighter-rouge">tacit behavior</code>에 대해서 수행하는 것이 가장 큰 contribution 입니다. 다만 실제 실험을 보았을 때에는 <code class="language-plaintext highlighter-rouge">tacit behavior</code>라고 표현하지 않아도 될 요소를 <code class="language-plaintext highlighter-rouge">attribute</code>로 삼아 연구를 수행한 것이 아쉬웠습니다. (<code class="language-plaintext highlighter-rouge">e.g., moving speed, instability of the movement</code>)</li> <li> <strong>RBA-Global</strong> 방법론은 기존에 수행해오던 PBL과 크게 다른 점은 없어보였고, feature function을 <code class="language-plaintext highlighter-rouge">attribute conditioned</code>하게 해준 것에 차이가 있는 것 같습니다. 그에 반해 <strong>RBA-Local</strong> 방법론은 human labelling을 필요로하지 않아 제가 느끼기에는 덜 수고스럽다는 점에서 유요하다고 여겨집니다. (50~100개 정도의 궤적에 대해 interaction을 거치면 수렴하는 듯한 양상을 보입니다.)</li> <li>코드가 공개되어 있어서, 실제로 돌려보고 구체적으로 각각의 parameter가 어떻게 적용되는지 공부하려고 합니다.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/aligndiff/">[paper-review] AlignDiff: Aligning Diverse Human Preferences via Behavior-customisable Diffusion Model</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cpl/">[paper-review] Contrastive Prefence Learning: Learning from Human Feedback without RL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/promptablebehaviors/">[paper-review] Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/exo-pbl/">[paper-review] User preference optimization for control of ankle exoskeletons using sample efficient active learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/stow/">[paper-review] Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joonhyung-lee/joonhyung-lee.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Joel Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
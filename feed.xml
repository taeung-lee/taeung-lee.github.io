<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://joonhyung-lee.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://joonhyung-lee.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-06T14:46:09+00:00</updated><id>https://joonhyung-lee.github.io//feed.xml</id><title type="html">Joel Lee</title><subtitle>Robotics Research Master&apos;s Student </subtitle><entry><title type="html">[paper-review] Text2Reaction : Enabling Reactive Task Planning Using Large Language Models</title><link href="https://joonhyung-lee.github.io//blog/2024/text2react/" rel="alternate" type="text/html" title="[paper-review] Text2Reaction : Enabling Reactive Task Planning Using Large Language Models"/><published>2024-03-24T00:00:00+00:00</published><updated>2024-03-24T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/text2react</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/text2react/"><![CDATA[<blockquote> <p>RA-L, 2024. [<a href="https://ieeexplore.ieee.org/document/10452794">Paper</a>]</p> <p>Zejun Yang , Li Ning, Haitao Wang, Tianyu Jiang, Shaolin Zhang, Shaowei Cui, Hao Jiang, Chunpeng Li, Shuo Wang and Zhaoqi Wang</p> <p>May. 05.</p> </blockquote> <div align="center"> <img src="/assets/img/text2react/overview.png" width="50%"/> <p>Fig. 1: Overview of Text2React.</p> </div> <h4 id="title">Title:</h4> <p>Text2Reaction : Enabling Reactive Task Planning Using Large Language Models (R-AL, 2024)</p> <h4 id="summary">Summary:</h4> <p>They propose Text2Reaction, an LLM-based framework enabling robots to continuously reason and update plans according to the latest environment changes.</p> <h4 id="contribution">Contribution:</h4> <div align="center"> <img src="/assets/img/text2react/flowchart.png" width="50%"/> <p>Fig. 2: Flowchart of Text2React.</p> </div> <div align="center"> <img src="/assets/img/text2react/reasoning-step.png" width="50%"/> <p>Fig. 3: Reasoning Steps of Text2React.</p> </div> <ul> <li>They present the Re-planning Prompt, which informs LLMs the basic principles of re-planning. <ul> <li>It fosters the gradual development of a current plan to a new one in a three-hop reasoning manner: cause analysis, consequence inference, and plan adjustment</li> </ul> </li> <li>OffPlanner: an LLM-based planner that generates initial plans</li> <li>On-Planner: another planner, which updates plans under the guidance of the re-planning prompts</li> </ul> <h4 id="thoughts">Thoughts:</h4> <ul> <li>Re-planning is an important part of the reactive robot. <ul> <li>They showed an LLM-based framework capable of comprehensively analyzing various feedback and continuously re-planning in response to environment changes.</li> </ul> </li> <li>They propose new evaluation metrics for the success rate of task replanning: Executability Rate(ER), Success weighted by Path Length(SPL).</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="paper-review/LLM"/><category term="LLM"/><category term="Object Manipulation"/><category term="Replanning"/><category term="RA-L"/><category term="2024"/><summary type="html"><![CDATA[paper review about Text2React]]></summary></entry><entry><title type="html">[paper-review] CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundational Model</title><link href="https://joonhyung-lee.github.io//blog/2024/copa/" rel="alternate" type="text/html" title="[paper-review] CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundational Model"/><published>2024-03-17T00:00:00+00:00</published><updated>2024-03-17T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/copa</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/copa/"><![CDATA[<blockquote> <p>ArXiv, 2024. [<a href="https://arxiv.org/abs/2403.08248">Paper</a>] [<a href="https://copa-2024.github.io/">Project</a>]</p> <p>Haoxu Huang<sup>2,3,4*</sup>, Fanqi Lin<sup>1,2,4*</sup>, Yingdong Hu<sup>1,2,4</sup>, Shengjie Wang<sup>1,2,4</sup>, Yang Gao<sup>1,2,4</sup> <sup>1</sup>Institute of Interdisciplinary Information Sciences, Tsinghua University. <sup>2</sup>Shanghai Qi Zhi Institute. <sup>3</sup>Shanghai Jiao Tong University. <sup>4</sup>Shanghai Artificial Intelligence Laboratory. <sup>*</sup> The first two authors contributed equally.</p> <p>Mar. 13.</p> </blockquote> <div align="center"> <img src="/assets/img/copa/copa-overview.png" width="50%"/> <p>Fig. 1: Overview of CoPa.</p> </div> <h4 id="title">Title</h4> <p>CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundational Model (2024, ArXiv)</p> <h4 id="summary">Summary:</h4> <ul> <li>They introduce a framework CoPa, which generates a sequence of 6-DoF end-effector poses for open-world robotic manipulation.They introduce a framework CoPa, which generates a sequence of 6-DoF end-effector poses for open-world robotic manipulation.</li> </ul> <h4 id="contributions">Contributions.</h4> <ul> <li>Task-Oriented Grasping Module <ul> <li>Firstly, they annotate the grasping object leveraging SoM method. (Coarse-Grained Object Grounding)</li> <li>Sequentially crop the image into the region of interest (ROI) of the grasped object. Annotate the grasp contact point in the pixel coordinates of the image. Take a sample grasp pose from GraspNet and match it to the annotated contact point. (Fine-grained part grounding)</li> </ul> </li> <li>Task-Aware Motion Planning Module <ul> <li>This module is used to obtain a series of post-grasp poses. Given the instruction and the current observation, they first employ a grounding module to identify task-relevant parts within the scene.</li> <li>Subsequently, these parts are modeled in 3D, and are then projected and annotated onto the scene image. Following this, VLMs are utilized to generate spatial constraints for these parts. Finally, a solver is applied to calculate the post-grasp poses based on these constraints.</li> </ul> </li> </ul> <h4 id="thoughts">Thoughts.</h4> <ul> <li>They presented their methodology in a very clear way: Combine (I) high-level task planning, which determines what to do next, and (ii) low-level robotic control, focusing on the precise actuation of joints. <ul> <li>Now the GPT-X model can be used in robotic tasks to think like a human.</li> </ul> </li> <li>They demonstrate the seamless integration with ViLa to accomplish long-horizon tasks. <ul> <li>The high-level planner generates a sequence of sub-goals, which are then executed by CoPa.</li> <li>The results show that CoPa can be easily integrated with existing high-level planning algorithms to accomplish complex, long-horizon tasks.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="paper-review/LLM"/><category term="VLMs"/><category term="Object Manipulation"/><category term="ArXiv"/><category term="2024"/><summary type="html"><![CDATA[paper review about CoPa]]></summary></entry><entry><title type="html">[paper-review] MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting</title><link href="https://joonhyung-lee.github.io//blog/2024/moka/" rel="alternate" type="text/html" title="[paper-review] MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting"/><published>2024-03-10T00:00:00+00:00</published><updated>2024-03-10T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/moka</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/moka/"><![CDATA[<blockquote> <p>ArXiv, 2024. [<a href="https://moka-manipulation.github.io/paper.pdf">Paper</a>] [<a href="https://moka-manipulation.github.io/">Project</a>]</p> <p>Kuan Fang<sup>*</sup>, Fangchen Liu<sup>*</sup>, Pieter Abbeel, Sergey Levine</p> <ul> <li>denotes equal contribution, alphabetical order Berkeley AI Research, UC Berkeley</li> </ul> <p>Mar. 05.</p> </blockquote> <div align="center"> <img src="/assets/img/moka/overview.png" width="50%"/> <p>Fig. 1: Overview of MOKA.</p> </div> <h4 id="title">Title</h4> <p>MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting (ArXiv, 2024)</p> <h4 id="summary">Summary</h4> <p>MOKA converts the motion generation problem into a series of visual question-answering problems that the VLM can solve.</p> <h4 id="method">Method</h4> <ul> <li>They introduce a point-based affordance representation that bridges the VLM’s prediction on RGB images and the robot’s motion in the physical world</li> <li>They refer to the physical interaction at each stage as a <em>subtask</em>, which includes interactions with objects in <em>hand</em> (e.g., lifting up an object, opening an drawer), interactions with environmental objects unattached to the robot (e.g., pushing an obstacle, pressing a button), and <em>tool</em> use which involves grasping a tool object to make contact with another object.</li> <li>Decompose the task into a sequence of feasible subtasks based on the free-form language description ( l )</li> <li>Then use VLM (GroundedSAM) to segment objects from the 2D image and overlay them (similar approach to SoM). Additionally, for each of the subtasks, the VLM is asked to provide the summary of the subtask instruction.</li> </ul> <h4 id="thoughts">Thoughts</h4> <ul> <li>This paper’s approach is similar idea with the previous approach of <em>VPI</em> paper. I read this paper with great interest.</li> <li>They have to lift all points from the 2D image into the 6D Cartesian space. So they only consider the cases where the waypoints are at the same height as the target point in most common table manipulation scenarios. <ul> <li>This point would limit the scope of this paper.</li> </ul> </li> <li>Since robust grasping relies on contact physics, they do not rely directly on the predicted grasp pose from the VLM, but use a grasp sampler that is closest to the response grasp pose.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="paper-review/LLM"/><category term="VLMs"/><category term="Set-of-Marks"/><category term="ArXiv"/><category term="2024"/><summary type="html"><![CDATA[paper review about MOKA]]></summary></entry><entry><title type="html">[paper-review] Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</title><link href="https://joonhyung-lee.github.io//blog/2024/rpdiff/" rel="alternate" type="text/html" title="[paper-review] Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement"/><published>2024-03-03T00:00:00+00:00</published><updated>2024-03-03T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/rpdiff</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/rpdiff/"><![CDATA[<blockquote> <p>CoRL, 2023. [<a href="https://arxiv.org/pdf/2307.04751.pdf">Paper</a>] [<a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">Project</a>]</p> <p>Anthony Simeonov, Ankit Goyal<sup>*</sup>, Lucas Manuelli<sup>*</sup>, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal<sup>*</sup><sup>*</sup>, Dieter Fox<sup>*</sup><sup>*</sup> Massachusetts Institute of Technology, NVIDIA Research, Improbable AI Lab</p> <p>Jul. 10.</p> </blockquote> <div align="center"> <img src="/assets/img/rpdiff/overview.png" width="50%"/> <p>Fig. 1: Overview of RPDiff Architecture.</p> </div> <h4 id="title">Title</h4> <p>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement (CoRL, 2023)</p> <h4 id="summary">Summary</h4> <p>Solving placement task in complex environment via diffusion-process.</p> <h4 id="method">Method</h4> <ul> <li>Iteratively de-noise the 6-DoF pose of the object until it satisfies the desired geometric relationship with the scene point cloud.</li> <li>Train a neural network (f_{\theta}) to predict an SE(3) transformation from the combined object-scene point cloud at each time step (t).</li> <li>They also using separate clasifier (h_{\phi}) to avoid “local optimal” solutions by scoring the predicted poses among 0~1.</li> <li>They use transformer for processing point clouds and making pose predictions: 1) identify important geometric parts within the object and the scene, 2) capture relationships that occur between the important parts of the object and the scene.</li> </ul> <h4 id="thoughts">Thoughts</h4> <ul> <li>The paper’s idea is intuitive. They consider (the position and the orientation) of target object and I think it is better than the other real-to-sim approaches.</li> <li>It seems like the paper of “6-dof graspnet” in the context of placement task because this paper consider the value of placement score among 0~1.</li> <li>But the author says that the limitation is “demonstration” data can only be easily obtained via scripted policies in simulation.</li> <li>And I think one more limitation is that they execute the predicted placement in open-loop. Adding the module about reacting or recovering from disturbance would be better.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="Diffusion Process"/><category term="Manipulation"/><category term="CoRL"/><category term="2023"/><summary type="html"><![CDATA[paper review about RPDiff]]></summary></entry><entry><title type="html">[paper-review] Fast-Replanning Motion Control for Non-Holonomic Vehicles with Aborting A*</title><link href="https://joonhyung-lee.github.io//blog/2024/staa/" rel="alternate" type="text/html" title="[paper-review] Fast-Replanning Motion Control for Non-Holonomic Vehicles with Aborting A*"/><published>2024-02-04T00:00:00+00:00</published><updated>2024-02-04T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/staa</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/staa/"><![CDATA[<blockquote> <p>IROS, 2022. [<a href="https://arxiv.org/pdf/2109.07775.pdf">Paper</a>] [<a href="https://www.youtube.com/watch?v=0Jm7cUAkKmQ">Video</a>]</p> <p>Marcell Missura<sup>1</sup>, Arindam Roychoudhury<sup>1</sup>, Maren Bennewitz<sup>1</sup> <sup>1</sup>All authors are with the Humanoid Robots Lab, University of Bonn, Germany. Contact: missura@cs.uni-bonn.de</p> <p>Jul. 21.</p> </blockquote> <div align="center"> <img src="/assets/img/staa/pathplanning.png" width="50%"/> <p>Fig. 1: Overview of ShortTerm Aborting A* (STAA*).</p> </div> <div align="center"> <div style="display: flex; justify-content: center; align-items: center;"> <div style="flex: 50%;"> <img src="/assets/img/staa/pathrtr.png" style="width: 100%;"/> <p>Fig. 2: Comparison between RTR and PathRTR.</p> </div> <div style="flex: 50%;"> <img src="/assets/img/staa/simulation-scene.png" style="width: 100%;"/> <p>Fig. 3: Simulation setup.</p> </div> </div> </div> <h3 id="summary">Summary</h3> <ul> <li>They present ShortTerm Aborting A* (STAA<em>): operates in locally bounded map (seems like dwa algorithm) and avoiding dynamic obstacles using short-term aborting a</em> algorithm. <ul> <li>They find a global path via Minimal Construct algorithm. Due to the superior performance of the Minimal Construct algorithm, they can afford to recompute the global path in every control cycle; 4.34ms on average.</li> <li>The STAA* motion planner operates in a bounded local map; (8m X 8m square), additionally, they define intermediate global goal pose within the local map.</li> <li>Finally, they locally plan a collision-avoidance path using short-term aborting a*.</li> </ul> </li> </ul> <h3 id="contributions">Contributions</h3> <ul> <li>Local map representation</li> <li>They propose local map representation by inflating convex polygons to avoid planning through too narrow passages the agent would not fit through. <ul> <li>Using <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8594124">Minimal Construct</a> algorithm</li> <li>They set the local goal path as $\tilde{G}$ to plan in the local map.</li> </ul> </li> <li>Short-Term Aborting A*: The centerpiece of this paper is specifically tailored for early abortion. <ul> <li>Like DWA, they sample actions in discrete ranges (in the paper they sample 7x7 actions) based on velocity specs: $(v_{\text{max}}, ~ w_{\text{max}})$</li> </ul> </li> <li>Then they predict the successor state using their dynamic model of the robot. <ul> <li>The radius of the arc is obtained by only using $(v_{\text{max}}, ~ w_{\text{max}})$.</li> </ul> </li> <li>They use SAT algorithm to check collisions. <ul> <li><a href="https://doraeul19.tistory.com/253?category=1066045">Separating Axis Theorem</a>: Collision Detection Using the Separating Axis Theorem</li> </ul> </li> <li>They evaluate heuristic functions leveraging RTR and PathRTR. <ul> <li>rotate-translate-rotate (RTR) time function: estimate the time needed to drive along a path to the intermediate goal and to attain the goal direction.</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Baselines <ul> <li>PD controller, DWA, STAA* (ours)</li> </ul> </li> <li>Evaluation of different heuristic functions <ul> <li>Path RTR (ours)</li> <li>Path Euclidean</li> <li>Dijkstra <ul> <li>when all agents are using STAA* as a planner, almost all collisions can be avoided no matter which heuristic is being used.</li> </ul> </li> </ul> </li> </ul> <h3 id="thoughts">Thoughts</h3> <ul> <li>The demo video shows powerful collision-free navigation and the visualization in the simulation is great.</li> <li>I think the main contribution of this paper is tailored for early abortion. <ul> <li>They also mentioned that one of the parameters is a trade-off between precision and computation time.</li> </ul> </li> <li>And the second-most contribution is a novel time evaluation in goal navigation: Path RTR. They officially released the code, but the implementation is in C++. So I have to study the code and hope to reimplement this algorithm in the mujoco environment.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Navigation"/><category term="Navigation"/><category term="Path Planning"/><category term="IROS"/><category term="2022"/><summary type="html"><![CDATA[paper review about STAA*]]></summary></entry><entry><title type="html">[paper-review] Reactive Base Control for On-The-Move Mobile Manipulation in Dynamic Environments</title><link href="https://joonhyung-lee.github.io//blog/2024/rbc/" rel="alternate" type="text/html" title="[paper-review] Reactive Base Control for On-The-Move Mobile Manipulation in Dynamic Environments"/><published>2024-01-28T00:00:00+00:00</published><updated>2024-01-28T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/rbc</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/rbc/"><![CDATA[<blockquote> <p>RA-L. [<a href="https://arxiv.org/pdf/2309.09393.pdf">Paper</a>] [<a href="https://benburgesslimerick.github.io/MotM-BaseControl/">Project Page</a>]</p> <p>Ben Burgess-Limerick<sup>1, 2</sup>, Jesse Haviland<sup>1, 2</sup>, Chris Lehnert<sup>1</sup>, Peter Corke<sup>1</sup> <sup>1</sup>Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia <sup>2</sup> CSIRO Data61, Brisbane, Australia</p> <p>Mar. 03</p> </blockquote> <div align="center"> <img src="/assets/img/rbc/rbc-overview.png" width="50%"/> <p>Fig. 1: Overview of Reactive Base Control.</p> </div> <h3 id="summary">Summary</h3> <ul> <li>This paper presents a reactive-based control method for mobile manipulation in dynamic environments, which significantly reduces the task time and improves the performance of the robot while avoiding static and dynamic obstacles.</li> <li>This approach contrasts with traditional methods where the base and manipulator are controlled separately, often resulting in increased task time due to the sequential completion of navigation and manipulation tasks​.</li> </ul> <div align="center"> <img src="/assets/img/rbc/rbc-staa-orientation.png" width="75%"/> <p>Fig. 2: Inclusion of orientation cost in STAA*.</p> </div> <div align="center"> <img src="/assets/img/rbc/rbc-pathptr.png" width="50%"/> <p>Fig. 3: Bezier path evaluation with PathRTR.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>The paper introduces a reactive base control system for mobile manipulation in dynamic environments, enabling tasks to be performed on-the-move and significantly reducing total task time. This work marks the first real-world demonstration of reactive manipulation with both static and dynamic obstacles, showcasing the method’s practical applicability and robustness. ​ <ul> <li>1) They used STAA* algorithm that plangs a collision-free global path by searching through a visibility graph, and then compuites the intersection of the global path and the borader of a local planning region to develop an intermediate goal. The most important addtion to STAA* is the inclusion of an orientation to the goal state. They consider orientation which enables poses to be achieved that smoothly connects the immediate target with the next goal.</li> <li>2) In A* graph search module, they employed PathRTR heuristic. And they combines it with an additional heuristic based on a Bezier curve. It encourages the exploration of states that can be connected to the goal through smooth curves. They said that a value of 25% results in curves that work well in experimental setup.</li> <li>3) Base Placement was optimized by simple cost functions by the weighted sum of two components: <ul> <li> \[C_i = C_{i,C} + 1.05 \cdot C_{i,N}\] <ul> <li>\(C_{i,C}\) is the estimated cost from <strong>robot to candidate.</strong></li> <li>\(C_{i,N}\) is the estimated cost from <strong>the candidate to the next target.</strong></li> </ul> </li> <li>The path cost for each candidate is evaluated using the PathRTR metric.</li> </ul> </li> <li>4) Arm Obstacle Avoidance: They leveraged Quadratic Program to calculate joint veloicited for a given desired end-effector and base velocity. The controller allows for slack in the achieved end-effector velocity.</li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I was already aware of a previous paper by this author, Manipulation On-The-Move, and it seems to be a follow-up to both this paper and an algorithm called STAA*.</li> <li>I think the contribution of this paper is to add a couple of heuristics to the existing algorithm, and I would have to start with the previous paper; the baseline controller implemented in this paper is all based on the previous paper as a reference. <ul> <li>STAA* / MotM / PathRTR</li> </ul> </li> <li>After reading the paper and watching the demo video, I was quite impressed with the holistic approach to motion planning for mobile-manipulator systems; so accurate and high performance.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Manipulation"/><category term="Mobile Manipulation"/><category term="Manipulation on-the-Move (MotM)"/><category term="Navigation"/><category term="RA-L"/><category term="2024"/><summary type="html"><![CDATA[paper review about Reactive Base Control]]></summary></entry><entry><title type="html">[paper-review] Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction</title><link href="https://joonhyung-lee.github.io//blog/2024/mole/" rel="alternate" type="text/html" title="[paper-review] Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction"/><published>2024-01-20T00:00:00+00:00</published><updated>2024-01-20T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/mole</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/mole/"><![CDATA[<blockquote> <p>ICLR. [<a href="https://openreview.net/forum?id=8HCARN2hhw">Paper</a>]</p> <p>Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca Monaci and Christian Wolf <sup>1</sup> <sup>1</sup>Naver Labs Europe, Meylan, France</p> <p>Sep. 29</p> </blockquote> <div align="center"> <img src="/assets/img/mole/overview.png" width="50%"/> <p>Fig. 1: Overview of MOLE.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>Navigability를 정의하고, 이 latent spatial representation을 학습하자.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li>Instead of learning to reconstruct, they cast the robotic perception task as a navigation task by a blind auxiliary agent generating a learning signal for the main agent.</li> </ul> <div align="center"> <img src="/assets/img/mole/concept.png" width="75%"/> <p>Fig. 2: Concept of MOLE.</p> </div> <div align="center"> <img src="/assets/img/mole/architecture.png" width="100%"/> <p>Fig. 3: Architecture of MOLE.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>They propose learning a latent spatial representation (i.e., Navigability). <ul> <li>This approach differs from traditional methods that rely on explicit scene reconstruction. Instead, it relies on a learned latent spatial representation of the environment for navigation.</li> </ul> </li> <li>They define representation \(r_t\) and optimize it based on its amount of information. This representation is refined by a blind auxiliary agent, which operates without direct visual observations, thereby testing and refining its utility for navigation.</li> <li>The author describes the difference between the two methods based on <strong>Behavior Cloning</strong> and <strong>Navigability</strong>. <ul> <li><strong>BC</strong> directly learns the main target policy from expert trajectories, approximating the desired optimal policy. <strong>Navigability</strong>, on the other hand, focuses on learning a representation that optimizes navigational skills (i.e., actions) like detecting navigable space and avoiding obstacles, rather than reconstructing the scene in detail.</li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I thought that the proposed method seems like a teacher-student network. The main policy (teacher) provides a latent spatial representation (teaching material) that the blind auxiliary agent (student) uses to learn navigational actions.</li> <li>The auxiliary agent’s performance in navigating using this representation gives feedback to improve the main agent’s ability to create effective latent representations. This method offers a more flexible and potentially robust way for robots to navigate diverse environments, especially where creating or relying on detailed maps is impractical or impossible.</li> <li>I think that It’s a notable step forward in the development of autonomous systems that can adapt to a wide range of real-world conditions.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Navigation"/><category term="Navigation"/><category term="GRU"/><category term="ICLR"/><category term="2024"/><summary type="html"><![CDATA[paper review about Mole]]></summary></entry><entry><title type="html">[paper-review] Statler: State-Maintaining Language Models for Embodied Reasoning</title><link href="https://joonhyung-lee.github.io//blog/2024/statler/" rel="alternate" type="text/html" title="[paper-review] Statler: State-Maintaining Language Models for Embodied Reasoning"/><published>2024-01-07T00:00:00+00:00</published><updated>2024-01-07T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/statler</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/statler/"><![CDATA[<blockquote> <p>Arxiv. [<a href="https://arxiv.org/pdf/2306.17840.pdf">Paper</a>] [<a href="https://statler-lm.github.io/">Project Page</a>]</p> <p>Takuma Yoneda<sup>*1</sup>, Jiading Fang<sup>*1</sup>, Peng Li<sup>*2</sup>, Huanyu Zhang<sup>*3</sup>, Tianchong Jiang<sup>3</sup>, Shengjie Lin<sup>1</sup>, Ben Picker<sup>3</sup>, David Yunis<sup>1</sup> Hongyuan Mei<sup>1</sup>, Matthew R. Walter<sup>1</sup> <sup>1</sup>TTI-Chicago, <sup>2</sup>Fudan University, <sup>3</sup>University of Chicago, *Equal Contribution</p> <p>Dec. 4</p> </blockquote> <div align="center"> <img src="/assets/img/statler/statler_teaser.png" width="100%"/> <p>Fig. 1: Overview of Statler.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>World State를 갱신해가며 LLM Reasoning을 수행하자.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존 LLM 방식은 자신이 뱉어준 Action과 Observation에 기인해 reasoning을 수행해왔음. <ul> <li>Traditional LLMs in robotics generate actions based only on prior actions and observations, lacking <strong>an explicit</strong> world state.</li> </ul> </li> <li>2개의 prompted LLM을 통해 world-state를 <strong>explicit</strong>하게 maintaining 하겠다는 의도. <blockquote> <p>Statler utilizes a pair of prompted LLMs: instructed by a few demonstrations, <strong>the world-state reader</strong> takes as input the user query, reads the estimated world state, and generates an executable action (e.g, a code snippet); instructed by another set of demonstrations, <strong>the world-state writer</strong> updates the world state estimate based on the action.</p> </blockquote> </li> <li>이를 통해 Long-Horizon LLM Interaction에서 기억을 소실하거나, misleading reasoning을 방지해준다고 함.</li> </ul> <h3 id="motivation">Motivation</h3> <div align="center"> <img src="/assets/img/statler/statler-motivation.png" width="100%"/> <p>Fig. 2: Motivation of Statler.</p> </div> <ul> <li>야바위 게임처럼, 컵 안의 물체는 계속해서 움직이게 되는데, 우리가 명시적으로 공의 움직임을 관찰하지는 못하지만, internal-representation에 의해 공이 어떠한 컵에 들어있는지 알 수 있다.</li> <li>이것에서 영감을 얻어, LLM이 관측하지 못하는 world-state에 대해 maintaining하는 방향으로 연구를 수행함.</li> </ul> <h3 id="methodology">Methodology</h3> <div align="center"> <div style="display: flex; justify-content: center; align-items: center;"> <div style="flex: 50%;"> <img src="/assets/img/statler/statler-world-reader.png" style="width: 100%;"/> <p>Fig. 3: World State Reader.</p> </div> <div style="flex: 50%;"> <img src="/assets/img/statler/statler-world-writer.png" style="width: 100%;"/> <p>Fig. 4: World State Writer.</p> </div> </div> </div> <ul> <li>World-State-Reader <ul> <li>reader는 현재 state를 고려한 action을 취해주는 LLM 역할.</li> <li>여기서 state가 update 되어야 하는 부분도 고려해서 답변을 만듦.</li> </ul> </li> <li>World-State-Writer <ul> <li>앞선 reader가 추론해낸 state에 기반해, state를 upadte하고, external memory에 저장함. (이것이 곧 current-state가 됨.)</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/statler/statler-example.png" width="50%"/> <p>Fig. 5: Example scenario about Statler.</p> </div> <h3 id="thoughts">Thoughts:</h3> <ul> <li>저자가 언급한 야바위 문제로 motivation을 얘기하느데, 이 점이 꽤나 재밌게 느껴졌습니다.</li> <li>지금 GPT-4V로 연구를 수행하며 느낀 점이, long-horizon interaction을 수행할 때에 기억을 소실한다는 것이었습니다.</li> <li>저자는 GPT-4 모델로 <strong>state-reader, state-writer</strong> 두 개로 역할을 나누어 이러한 기억 소실을 방지해내고자 했습니다. <ul> <li>appendix도 읽어보았으나, prompt에 대해 novel한 부분을 찾지는 못했습니다. world-state를 template에 맞게 뱉어주고, 이에 기반해 world-state를 업데이트 시켰다고 하는데, 단순히 이것 만으로 기억 소실을 개선시켰다는 점이 신기합니다.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/LLM"/><category term="LLM"/><category term="Reasoning"/><category term="Arxiv"/><category term="2024"/><summary type="html"><![CDATA[paper review about Statler]]></summary></entry><entry><title type="html">[paper-review] Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</title><link href="https://joonhyung-lee.github.io//blog/2023/promptablebehaviors/" rel="alternate" type="text/html" title="[paper-review] Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences"/><published>2023-12-24T00:00:00+00:00</published><updated>2023-12-24T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/promptablebehaviors</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/promptablebehaviors/"><![CDATA[<blockquote> <p>Arxiv. [<a href="https://arxiv.org/pdf/2312.09337.pdf">Paper</a>] [<a href="https://promptable-behaviors.github.io/">Project Page</a>]</p> <p>Minyoung Hwang<sup>1</sup>, Luca Weihs<sup>1</sup>, Chanwoo Park<sup>2</sup>, Kimin Lee<sup>3</sup>, Aniruddha Kembhavi<sup>1</sup>, Kiana Ehsani<sup>1</sup> <sup>1</sup>PRIOR @ Allen Institute for AI, <sup>2</sup>Massachusetts Institute of Technology, <sup>3</sup>Korea Advanced Institute of Science and Technology</p> <p>Dec. 14</p> </blockquote> <div align="center"> <img src="/assets/img/promptablebehaviors/overview.png" width="100%"/> <p>Fig. 1: Overview of PromptableBehavior.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>promptable navigation behavior 연구를 선보였다.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존의 Embodied AI에서 hand-crafted reward design이 어려웠음. <ul> <li>Novel framework를 제시함: simplify the reward design process</li> </ul> </li> <li>3개의 interaction type을 통해 human preference를 추론했다. <ul> <li>(1) human demonstration, (2) preference feedback on trajectory comparison, (3) language instructions</li> </ul> </li> <li>ProcTHOR, RoboTHOR에서 실험을 수행함.</li> </ul> <h3 id="problem-formulation">Problem Formulation</h3> <ul> <li>가정하고 있는 점은 아래와 같음. <ul> <li>Human preference remains constant over time</li> <li>Each human preference is captured through a linear combination of multiple objectives in the environment</li> </ul> </li> <li>매 timestep t 마다 agent는 RGB observation \(o_t\)에 기반한 action \(a_t\)를 뱉어낸다. <ul> <li>action은 <code class="language-plaintext highlighter-rouge">[MoveAhead, RotateRight, RotateLeft, Done, LookUp, LookDown]</code>이 있음.</li> </ul> </li> <li>저자는 여기서 <code class="language-plaintext highlighter-rouge">agent’s (navigation) behavior</code>에 집중해, preference로 표현해주려고 함.</li> </ul> <h3 id="methodology-multi-objective-reinforcement-learning-morl">Methodology: Multi-Objective Reinforcement Learning (MORL)</h3> <div align="center"> <img src="/assets/img/promptablebehaviors/architecture.png" width="100%"/> <p>Fig. 2: Architecture of PromptableBehavior.</p> </div> <h4 id="build-scene-representation">Build Scene Representation</h4> <ul> <li>multiple objective를 갖는 policy를 학습함. (conditioned on a <strong>reward weight vector</strong>) <ul> <li>(이는 <a href="https://arxiv.org/pdf/2211.09960.pdf">Ask4Help</a> 논문에서 영감을 얻었다고 함)</li> </ul> </li> <li><strong>reward weight vector</strong>를 human preference에 일치하게 infer하도록 함. 이러한 <strong>reward weight</strong>에 대해서 추가적인 fine-tuning 없이 <strong>preference 조작</strong>이 가능함.</li> <li>저자가 제시하는 Promptable Behaviors는 2가지 단계로 이루어짐. <ul> <li>(1) Training a promptable multi-objective policy</li> <li>(2) Capturing the agent’s desired behavior through interactions.</li> </ul> </li> <li>Image encoding에는 CLIP 모델을 사용함.</li> <li>Reward weight encoder로는 feed-forward neural network(FFNN)을 활용해, \(K \cdot 12\)-dim latent codebook으로 표현함. <ul> <li>\(r^{\mathbf{w}}=\mathbf{w^{\intercal}r}\), <ul> <li> <table> <tbody> <tr> <td>\(\mathbf{w}\): randomly sampled from \(K\)-dim simplex $$\Delta_K={\mathbf{w} \in \mathbb{R}^{K}_{+}</td> <td>~</td> <td> </td> <td>\mathbf{w}</td> <td> </td> <td>_{1}=1}$$</td> </tr> </tbody> </table> </li> <li>기존의 연구들은 \(\mathbf{w}\)가 pre-defined 되어 있었지만, 저자는 이를 randomly exploration하겠다는 목적임. (그리고 이 reward weight vector \(\mathbf{w}\in\Delta_K\)인 user’s true preference로 표현된다고 가정한다.)</li> </ul> </li> </ul> </li> <li>Navigation policy: DD-PPO 모델로 수행함.</li> </ul> <h4 id="types-of-interaction-reasoning-through-interactions">Types of Interaction: reasoning through interactions</h4> <p>(1) Human Demonstration</p> <ul> <li>demonstrated action과 action distribution from policy \(\pi\) 간의 log-likelihood loss로 계산함.</li> </ul> <p>(2) Trajectory Comparison</p> <ul> <li>일반적인 Bradley-Terry 구조로 수행함.</li> </ul> <p>(3) Language Instruction</p> <ul> <li>사용자가 제시하는 <strong>task description과 definition of objective</strong>를 토대로 ChatGPT가 <strong>optimal reward weight vector</strong> 값을 뱉어주도록 하였음. (In-Context Learning, Chain-of-Thought로 수행함.)</li> <li>Objective sets: <code class="language-plaintext highlighter-rouge">time efficiency, path efficiency, house exploration, safety</code></li> </ul> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/promptablebehaviors/result.png" width="100%"/> <p>Fig. 3: Results of PromptableBehavior.</p> </div> <h3 id="thoughts">Thoughts:</h3> <ul> <li>제가 생각하는 preference의 정의는 해당 논문에서 표현하듯, agent behavior에 기반한 것과 더 근접하다고 생각합니다.</li> <li>현재 저도 진행 중인 연구에 대해서 GPT-4V 모델이 human preference를 추론해주는 것만으로 contribution을 내세우기에는 어려움이 있을 것 같다고 여겨집니다. <ul> <li>저도 계속 구상해오던 점은, 해당 논문에서 수행한 것처럼 <strong>(1) 어떠한 reward policy를 학습하거나, (2) GPT가 preference weight를 뱉어주도록</strong> 하는 방향으로 수행하면 어떨까 라는 생각이 들었습니다. manipulator scene에 대해서도 충분히 arm motion에 대한 objective set을 정의해, 그에 따른 trajectory 결과도 보여줄 수 있을 것이라 생각합니다.</li> </ul> </li> <li>기존의 다른 preference-based RL 논문과 비교해 이 논문이 새로웠던 점은, K-dim reward weight에 대해 학습해주는 과정이라고 생각합니다. <ul> <li>이를 통해 기존에는 pre-defined and fixed w에 대해 수행된 것과 다르게, reward weight에 대해 exploration을 수행할 수 있었다고 생각합니다. (+ codebook representation)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/PBL"/><category term="PBL"/><category term="LLM"/><category term="Reasoning"/><category term="NeurIPS-W"/><category term="2023"/><summary type="html"><![CDATA[paper review about Dream2Real]]></summary></entry><entry><title type="html">[seminar] Robot Learning Day</title><link href="https://joonhyung-lee.github.io//blog/2023/robot-leraning-day/" rel="alternate" type="text/html" title="[seminar] Robot Learning Day"/><published>2023-12-22T00:00:00+00:00</published><updated>2023-12-22T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/robot-leraning-day</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/robot-leraning-day/"><![CDATA[<blockquote> <p>Robot Learning Day 세미나 내용을 기록했습니다.</p> </blockquote> <h3 id="이경재-교수님-세미나">이경재 교수님 세미나</h3> <p><strong>Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</strong></p> <ul> <li>RLHF <ul> <li>Design Reward Function from Human Feedback</li> </ul> </li> <li>Preference: Starts with assuming. <ul> <li>Deterministic: underlined human’s return value</li> <li>Stochastic: Bradley-Terry model</li> </ul> </li> <li>Cognitive Load and Decision Fatigue <ul> <li>lead to incorrect, noisy labels</li> </ul> </li> </ul> <p>⇒ Using <strong>transitivity</strong>: 한 번의 feedback으로 많은 양의 preference label을 만들자.</p> <ul> <li> <table> <tbody> <tr> <td>Transitivity: Set of items \(\mathcal{A}\), a &gt; b</td> <td>b &gt; c ⇒ a &gt; c</td> </tr> </tbody> </table> </li> <li>Linear Stochastic Transitivity <ul> <li>Bradley-Terry</li> </ul> </li> <li>Sequential Pairwise Comparison <ul> <li>Preference Labelling 이후에, <strong>하나의 component를 남겨놓음.</strong> <ul> <li><code class="language-plaintext highlighter-rouge">Increasing sequence</code>: j-th index 까지 increase하는 상황을 가정. (Average case.)</li> </ul> </li> </ul> </li> <li>Root Pairwise Comparison <ul> <li>지금까지 수행한 것 중 <strong>가장 선호하는 것을 Back-tracking.</strong></li> <li>case가 훨씬 단순하며, augmentation의 양이 더욱 많음. (Best, Worst Case) ⇒ 새로운 trajectory data에 대해서 많은 양의 preference label이 추가됨. <ul> <li>보라색 line은 transitivity를 통해 얻어낼 수 있는 데이터를 뜻함.</li> </ul> </li> </ul> </li> </ul> <h4 id="is-it-always-beneficial">Is it always beneficial?</h4> <ul> <li>반드시 data dependency가 생기게 됨.</li> <li>Dependency Graph \(G\) <ul> <li>error bound 유도가 이미 되어 있음.</li> <li>\(\Delta G\): dependency graph <ul> <li>edge의 수가 해당 값인 degree를 의미함.</li> </ul> </li> <li>Every M roond 때에 dependency graph를 끊어줌.</li> </ul> </li> </ul> <h3 id="오윤선-교수님-세미나">오윤선 교수님 세미나</h3> <ul> <li>MAPF: Multi-Agent Path Finding <ul> <li>Conflict-Based Search: Optimality를 보장함.</li> <li>Priority-Based Search: 계획 속도 측면에서 효율적.</li> </ul> </li> <li>Robot Safety conflict <ul> <li>Cycle Conflict가 발생하게 됨. 서로 갇히게 됨.</li> </ul> </li> <li>Implicit language instruction이 제공되었을 때에 적절히 수행할 수 있는 연구를 수행 중임. <ul> <li><code class="language-plaintext highlighter-rouge">Get me something to tighten</code></li> <li><code class="language-plaintext highlighter-rouge">Get me something to wear on</code> <ul> <li>Grasp pose를 retrieval</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">“대부분의 작업 계획은 상식을 기반으로 수행됨.”</code></li> <li>LLM + Feasibility를 확보하는 연구.</li> <li>Scene Graph를 Text-embedding으로 표현.</li> <li>VLM-based Task Planning <ul> <li><code class="language-plaintext highlighter-rouge">Tell me the order of which objects to pick up.</code></li> <li><strong>LaVIN</strong></li> </ul> </li> </ul> <h3 id="강민재-연구원-논문-소개">강민재 연구원 논문 소개</h3> <h4 id="object-rearrangement-planning-for-target-retrieval">Object Rearrangement Planning for Target Retrieval</h4> <ul> <li>Target 회수 문제 <ul> <li>Occlusion + Collision</li> <li>Unoccupied space is limited</li> </ul> </li> <li>기존의 가정: 모든 size를 알고 있음 + 모든 방향에서 잡을 수 있음. <ul> <li>shape+position을 추정 + grasping 자세가 제한됨</li> <li>NP-Hard ⇒ Sequential sub-problem <ul> <li>What is sub-problem?</li> <li>How can we solve this?</li> <li>Sequential sub-problem: able to solve?</li> </ul> </li> </ul> </li> <li>TSAD: Tree Search with Approaching Direction <ul> <li>Ref: GP3 paper; <strong>optimal theta</strong>를 얻게 됨.</li> <li>이러한 theta set을 <strong>Smallest Rearrangement Set</strong>으로 정의함. <ul> <li>이 집합이 공집합이 되면, collision 없이 물체를 꺼내올 수 있다는 것임.</li> </ul> </li> <li>Point Cloud-Based Simulation; <ul> <li>pointcloud를 움직이며 간접적으로 이해하려고 함. <ul> <li>Prehensile Decision Network</li> <li>Task-Specific State Reward Function</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="홍민의-연구원-논문-소개">홍민의 연구원 논문 소개</h3> <h4 id="diffused-task-agnostic-milestone-planner">Diffused Task-Agnostic Milestone Planner</h4> <p>Goal-conditioned RL</p> <ul> <li>Temporally-Sparese Milestone: sub-goal을 tracking 하게끔 함</li> <li>Predict milestones only once.</li> </ul> <h4 id="권오빈-연구원-논문-소개">권오빈 연구원 논문 소개</h4> <ul> <li>Grid-based Visual Navigation <ul> <li>RNR-Map: Renderable Neural Radiance Map</li> </ul> </li> </ul> <h3 id="기호건-연구원-논문-소개">기호건 연구원 논문 소개</h3> <h4 id="sdf-based-graph-convolutional-q-network">SDF-Based Graph Convolutional Q-Network</h4> <ul> <li>Find Action sequence <ul> <li>Sign-Distance Function을 기반으로 학습. <ul> <li>Fast Marching Method</li> <li>MDP의 state로 정의됨.</li> </ul> </li> </ul> </li> <li>SDFGCN <ul> <li>Scene Graph Generation <ul> <li>Init image / Final image를 기반으로, <strong>어떠한 방향으로 밀어낼지</strong>를 학습함.</li> <li>Complete Sub-graph를 생성함.</li> <li><strong>SDF representation이 나름 표현력이 뛰어남.</strong></li> </ul> </li> </ul> </li> </ul> <h3 id="오정우-연구원-논문-소개">오정우 연구원 논문 소개</h3> <h4 id="scan-socially-aware-navigation-using-mcts">SCAN: Socially-Aware Navigation Using MCTS</h4> <ul> <li>plan paths without considering future states or human-robot interaction <ul> <li>HRI must be considered!</li> </ul> </li> <li>MCTS로 multi-traj를 sampling <ul> <li>Simulation에서 Value-function 계산</li> <li>Real-world Deploy</li> <li>High-level planner를 만듦. <ol> <li>Cost-aware RRT-Star: Tree Search</li> <li>Value Prediction: <ol> <li>MCTS-sampled trajectory</li> <li>Local goal</li> <li>Global goal</li> </ol> </li> <li>Candidate Selection based on Value</li> </ol> </li> </ul> </li> <li>Evaluation <ul> <li>Task: Point Goal Navigation</li> <li>SANS: Socially-aware navigation score <ul> <li>CR: goal까지 온전하게 수행될 percentage</li> <li>SP: speed</li> <li>PE: Path efficiency</li> <li>SF: Safety score, LIDAR 값 중 최소값을 기준으로 safety-verification</li> <li>ST: Stability score; 이러한 Unsafe state에서 얼마나 빠르게 stable state로 빠져나왔는지.</li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="seminar"/><category term="Robotics and Learning"/><summary type="html"><![CDATA[seminar summary about Robot Learning Day]]></summary></entry></feed>